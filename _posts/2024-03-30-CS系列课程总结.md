---
title: CS系列课程总结
date: 2024-03-30 16:01:00 +0800
categories: [tools, notes]
tags: [tools]     # TAG names should always be lowercase
math: true
mermaid: true
img_path: /commons/2024-03-30-CS系列课程总结/
author: hupenghui
---

<!-- markdownlint-capture -->
<!-- markdownlint-disable -->
> 学习的理论与计算机中实际运行的代码之间总是感觉有间隙，大部分介绍文档都是模棱两可，他们也知道有区别，但是就是不敢说或者说他们也不知道怎么描述这个问题。`CS`系列课程就很好，很清晰，不同的概念就是会明确表示这是不同的概念，不会因为概念上的多义而让你模棱两可。比如线性代数理论，书中学习的是线性代数理论，但是计算机中实现的张量代数理论，这两者是不同的，`CS`就非常明确的说这是不同的。
{: .prompt-tip }
<!-- markdownlint-restore -->

## 强化学习基础

$$
\begin{equation}
  \nabla_{\theta}\sum_{s}\sum_{a}P(s)\pi(a|s)r(s,a)=\sum_{s}\sum_{a}\nabla_{\theta}[P(s)\pi(a|s)]r(s,a)
  \label{eq:series}
\end{equation}
$$

1. [Deep Reinforcement Learning](https://rail.eecs.berkeley.edu/deeprlcourse/)
    - 提出张量代数()这个名词，解决了线性代数应用于高维张量上的困境。
    - 策略梯度的分解公式分为了两步，第一步先确定对单个奖励还是单个概率分布进行求和，第二步才是对求和后的结果进行求偏导，才能正确对策略梯度进行分解。从奖励的分解角度看，很容易得到策略梯度的表达式（如果从概率的分解角度来看）为.
    - 书中的分解方法会导致最后一个奖励所对应的概率是全的，第一个奖励对应的概率是只有第一个动作，第二个奖励对应的只有第一二两个动作的概率。
    - 按照我脑子中的分解的话，第一个奖励函数对应的动作只有第一个动作，第二个奖励对应的只有第一二个动作的概率，妈的，两种分解是一样的，也就是说因果就是我的那种分解方法。哭了！因果性跟我用先求和再求导得到的结果是一致的，可是为什么因果性这么难以理解？
    - 强化学习的目标是发现新的动作空间，而深度学习的目标是希望能够模拟人的能力。比如说分类，人也能做就是太累了，算的慢，希望机器实现人一样的分类能力，而另一类问题是很难将人能做的表述出来。emmm，这些表述性的话语很难对齐，还是看表达式吧。
    - 解决这样的问题我们需要什么样的模型？分模块还是一个端到端？他要具有哪些功能？感知环境，选择动作。这就可以用深度强化学习实现。深度学习表示能力强，强化学习可以解决决策问题。
    - 面临一个问题我们有两种解决办法：1.理解问题、设计解决规则。2.设置为机器学习问题。
    - 最近在ai领域的发展有哪些？GPT，图像生成技术。这些发展背后的技术是什么？学习样本的概率分布，也就是监督学习的逻辑。
    - 强化学习与上面的想法不同，他是面向决策的。强化学习并不能发现新的动作只是发现新的交互路线。新的交互图谱。这个交互图谱对博弈双方是影响最大的。
    - 也可以说强化学习就是学习如何去做，而深度学习去学习如何去说。一个只会说，一个会做。强化学习可以用于很多领域。

\eqref{eq:series} 对于回报均值的计算采用了将回报分解为奖励的方式 $r(\tau)=\sum_{i=1}^n r(s_i,a_i)$:

$$
\begin{equation}
  {R}=\sum_{s_1}\sum_{a_1}p(s_1)\pi(a_1|s_1)r(s_1,a_1)+\sum_{s_1}\sum_{a_1}\sum_{s_2}\sum_{a_2}p(s_1)\pi(a_1|s_1)p(s_2|s_1,a_1)\pi(a_2|s_2)r(s_2,a_2)+\cdots
  \label{eq:mean_return}
\end{equation}
$$

The mean of the return will be \eqref{eq:mean_return}:

$$
\begin{equation}
  {R}=\sum_{s_1}\sum_{a_1}p(s_1)\frac{\pi(a_1|s_1)}{\pi_o(a_1|s_1)}\pi_o(a_1|s_1)r(s_1,a_1)+\sum_{s_1}\sum_{a_1}\sum_{s_2}\sum_{a_2}p(s_1)\pi(a_1|s_1)p(s_2|s_1,a_1)\pi(a_2|s_2)\frac{\pi_o(a_1|s_1)\pi_o(a_2|s_2)}{\pi_o(a_1|s_1)\pi_o(a_2|s_2)}r(s_2,a_2)+\cdots
  \label{eq:importance_sample}
\end{equation}
$$

if we use importance sample, then the mean of the return will be \eqref{eq:importance_sample}:

$$
\begin{equation}
  {R}=\sum_{s_1}\sum_{a_1}p(s_1)\pi_o(a_1|s_1)\frac{\pi(a_1|s_1)}{\pi_o(a_1|s_1)}r(s_1,a_1)+\sum_{s_1}\sum_{a_1}\sum_{s_2}\sum_{a_2}p(s_1)\pi_o(a_1|s_1)p(s_2|s_1,a_1)\pi_o(a_2|s_2)\frac{\pi(a_1|s_1)\pi(a_2|s_2)}{\pi_o(a_1|s_1)\pi_o(a_2|s_2)}r(s_2,a_2)+\cdots
  \label{eq:factor_resequence}
\end{equation}
$$

We can change the sequence of the factor in above equation \eqref{eq:factor_resequence}.
