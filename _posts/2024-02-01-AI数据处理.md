---
title: AI数据处理
date: 2024-02-01 16:09:00 +0800
categories: [tools, notes]
tags: [tools]     # TAG names should always be lowercase
math: true
mermaid: true
img_path: /commons/2024-02-01-数据处理/
author: hupenghui
---

> 进行人工智能相关开发，很重要的一点就是数据的准备与处理，而现实中的数据处理难点主要有两个：第一，数据并不全是以数字的形式存在的，很多的是以文字和图片的形式存在的，我们需要对这些计算机识别不了的数据手动转化为数字（图片还好说，读进来就是数字形式，主要是文字难以处理）。第二，现实中大部分业务数据都是存储在数据库中，需要我们使用sql指令从数据库中读取数据，然后才能使用python相关编程语言进行处理（使用sql对表数据进行处理，使用pandas对表格数据处理，使用numpy对数字数据处理）。因此第一部分介绍如何从数据库中读取数据。第二部分介绍如何对数据格式和内容进行转换。

## 数据准备的要点

自然语言处理中，大部分训练数据都是以json的形式构造的。json的结构很简单，就是字典的形式，有键有值。但是关键是：键应该怎么写？值应该怎么构造？多个样本数据怎么放在一起？这些才是关键，不要因为一点熟悉的知识就忘记了其他重要的知识。存储还是按照文本形式存储的，换行记得加，然后每一行都是一个json库dump下的字典。

从数据库中提取数据关键操作是把多个表链接起来，通过关键字来实现多个表之间的关联，可能跟我们最终需要的数据格式并不一致，但是一定包含我们所需要的所有数据。

然后通过pandas对数据格式进行调整，转存为json格式。

然后再从json格式转化为numpy数组格式，这个步骤往往是在继承datasets类的过程中实现的，即继承datasets类并实现数据格式的转换。然后我们可以得到具有标准形式的数据集，然后再通过实现dataloader类来把数据加载进dataloader。dataloader往往需要跟enumerate配合使用，这样我们可以逐批次的使用训练数据。参考paddle飞桨手册。

## 从数据库中读取数据

首先是从数据库中提取多个表的通过关键字链接起来的数据：参考美团实习经历

```sql
SELECT query_text,
       doc_text,
       query_geohash,
       geohash_doc,
       is_geo_query,
       CASE WHEN doc_text LIKE concat('%',query_text,'%') THEN 1
            ELSE 0
             END AS is_contain
  FROM (
        SELECT *
          FROM (
                SELECT query_text,
                       doc_text,
                       query_geohash,
                       geohash_doc,
                       is_geo_query,
                       row_number () OVER(PARTITION BY query_text ORDER BY rand()) AS rn
                  FROM (
                        SELECT DISTINCT qm.global_id AS global_id,
                               qm.query_text AS query_text,
                               qm.doc_text AS doc_text,
                               qm.geohash_doc AS geohash_doc,
                               qm.query_geohash AS query_geohash,
                               c_7.is_geo_query AS is_geo_query
                          FROM (
                                SELECT log.search_keyword AS query_text,
                                       poi_dim.poi_name AS doc_text,
                                       poi_dim.geohash AS geohash_doc,
                                       log.geohash AS query_geohash,
                                       log.global_id AS global_id,log.partition_date as partition_date
                                  FROM (
                                        SELECT item_id,
                                               search_keyword,
                                               geohash,
                                               partition_date,
                                               global_id
                                          FROM mart_ssai.topic_search_result_item_funnel_d_d
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND item_id_system='mtPoi'
                                           AND valid_click_cnt >1
                                           AND search_type='mt_home_search'
                                       ) AS log
                                  JOIN (
                                        SELECT *
                                          FROM mart_ssai.dim_poi_common
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND poi_name <> 'NULL'
                                       ) AS poi_dim
                                    ON log.item_id=poi_dim.poi_id
                                   AND log.partition_date=poi_dim.partition_date
                                 WHERE log.search_keyword <> poi_dim.poi_name
                               ) qm
                          JOIN (
                                SELECT global_id AS global_id,dt,
                                       CASE WHEN log LIKE "%loction_info:%" THEN 1
                                            ELSE 0
                                             END AS is_geo_query
                                  FROM log.dataapp_search_query_analysis
                                 WHERE dt between '20231123' and '20231127'
                                   AND SOURCE = 'com.sankuai.mobile.search.mergeservice'
                                   AND UUID != ''
                                   AND length(query) > 0
                               ) c_7
                            ON c_7.global_id = qm.global_id
                       ) c_8
               ) tmp
         WHERE rn <=5
       ) s3
```

上面提取出的数据只会在sql中存在，我们需要转存到文件中，就需要使用python。

```python
from pyspark import SparkConf
from pyspark.sql import SparkSession
sparkConf = SparkConf() \
    .setAppName("Jupyter") \
    .setMaster("yarn") \
    .set('spark.yarn.dist.archives', 'viewfs://hadoop-meituan/user/hadoop-mlp-jupyter/mtjupyter/archives/scipy-notebook-f8ea9eb.zip#ARCHIVE') \
    .set("spark.yarn.archive", "viewfs://hadoop-meituan/user/hadoop-hdp/notebook/spark-jars/spark_libs.zip") \
    .set("spark.yarn.queue", "root.zw03.hadoop-search.offline") \
    .set("spark.shuffle.service.enabled", "true") \
    .set("spark.dynamicAllocation.enabled", "true") \
    .set("spark.dynamicAllocation.minExecutors", "10") \
    .set("spark.dynamicAllocation.maxExecutors", "300") \
    .set("spark.executor.memory","4G") \
    .set("spark.driver.memory","7G") \
    .set("spark.input.dir.recursive","true") \
    .set("spark.sql.ignore.existed.function.enable","true") \
    .set("spark.yarn.maxAppAttempts", "1") \
    .set("spark.noResourceMaxInterval","86400s") \
    .set("spark.scheduler.mode", "FAIR") \
    .set("spark.executor.cores","2") \
    .set("spark.yarn.executor.memoryOverhead","1024") \
    .set("spark.yarn.driver.memoryOverhead","1024") \
    .set('spark.hadoop.hive.mt.renew.token.enable', 'true') \
    .set('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')
spark = SparkSession.builder.config(conf=sparkConf).enableHiveSupport().getOrCreate()
from pyspark.sql import HiveContext
sqlCtx = HiveContext(spark)
df = sqlCtx.sql("""SELECT query_text,
       doc_text,
       query_geohash,
       geohash_doc,
       is_geo_query,
       CASE WHEN doc_text LIKE concat('%',query_text,'%') THEN 1
            ELSE 0
             END AS is_contain
  FROM (
        SELECT *
          FROM (
                SELECT query_text,
                       doc_text,
                       query_geohash,
                       geohash_doc,
                       is_geo_query,
                       row_number () OVER(PARTITION BY query_text ORDER BY rand()) AS rn
                  FROM (
                        SELECT DISTINCT qm.global_id AS global_id,
                               qm.query_text AS query_text,
                               qm.doc_text AS doc_text,
                               qm.geohash_doc AS geohash_doc,
                               qm.query_geohash AS query_geohash,
                               c_7.is_geo_query AS is_geo_query
                          FROM (
                                SELECT log.search_keyword AS query_text,
                                       poi_dim.poi_name AS doc_text,
                                       poi_dim.geohash AS geohash_doc,
                                       log.geohash AS query_geohash,
                                       log.global_id AS global_id,log.partition_date as partition_date
                                  FROM (
                                        SELECT item_id,
                                               search_keyword,
                                               geohash,
                                               partition_date,
                                               global_id
                                          FROM mart_ssai.topic_search_result_item_funnel_d_d
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND item_id_system='mtPoi'
                                           AND valid_click_cnt >1
                                           AND search_type='mt_home_search'
                                       ) AS log
                                  JOIN (
                                        SELECT *
                                          FROM mart_ssai.dim_poi_common
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND poi_name <> 'NULL'
                                       )AS poi_dim
                                    ON log.item_id=poi_dim.poi_id
                                   AND log.partition_date=poi_dim.partition_date
                                 WHERE log.search_keyword <> poi_dim.poi_name
                               ) qm
                          JOIN (
                                SELECT global_id AS global_id,dt,
                                       CASE WHEN log LIKE "%loction_info:%" THEN 1
                                            ELSE 0
                                             END AS is_geo_query
                                  FROM log.dataapp_search_query_analysis
                                 WHERE dt between '20231123' and '20231127'
                                   AND SOURCE = 'com.sankuai.mobile.search.mergeservice'
                                   AND UUID != ''
                                   AND length(query) > 0
                               ) c_7
                            ON c_7.global_id = qm.global_id
                       ) c_8
               ) tmp
         WHERE rn <=5
       ) s3""")
df.write.mode("overwrite").options(header="true").csv("viewfs://hadoop-meituan/user/hadoop-search/qikena/train_mt_poi_1123-1127")
```

可以将上述数据以csv的形式存储下来。这个select语句会创建一个表格数据，表头就是上面的属性名字，内容就是表的内容。当把它存下来时，格式就是原来的表格形式，不过是中间的分隔符用逗号来代替。
也就是说原来是：

| query_text | doc_text | query_geohash | geohash_doc | is_geo_query |
|:-----------|:---------|:--------------|:------------|-------------:|
| query      | doc      | query_hash    | doc_hash    | bool         |

然后存储下来就是：`query_text,doc_text,query_geohash,geohash_doc,is_geo_query\nquery,doc,query_hash,doc_hash,bool`。当然可能还需要进行其他处理，比如把spark的多个文件合并。

```bash
#查看文件架构
!hdfs dfs -ls -h viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230
#删除同名文件
!hdfs dfs -ls -h viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
!hdfs dfs -rm -r viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
#复制文件
!hdfs dfs -cp viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/part-00000-05587d70-c041-4775-a226-a67646a67d35-c000.csv viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
#扩充文件
!hdfs dfs -ls viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/part-*.csv | grep -v 'part-00000-05587d70-c041-4775-a226-a67646a67d35-c000.csv' | awk '{print $NF}' | while read FILE; do hdfs dfs -cat "$FILE" | tail -n +2 | hdfs dfs -appendToFile - viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv;done
#拉取文件
!hdfs dfs -copyToLocal viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/train_1222-1230_total.csv /home/hadoop-search/dolphinfs_hdd_hadoop-search/hupenghui/data/train_1222-1230_total.csv
```

## 使用python对表格数据进行转存（csv-json）

然后是从链接数据中转存为json格式的数据。参考美团实习经历

```python
# -* -coding:utf-8 -*-

import json
import codecs
import pandas as pd
import random


def load_sigle_df(path,is_need_contain=True):
    train_data_csv = pd.read_csv(path, on_bad_lines='skip')
    doc = train_data_csv
    doc = doc.dropna(subset=["doc_text"])
    doc = doc.dropna(subset=["query_text"])
    if is_need_contain:
        is_contain_check = doc.apply(lambda x: is_contain_text_check(x), axis=1)
        doc["is_contain"] = is_contain_check
        doc.to_csv("../../data/train/train_mt_poi_7_is_contain.csv")
    doc.fillna('NULL', inplace=True)
    sample_dup_data = doc[doc["is_contain"] == '1'].sample(frac=0.3)
    all_doc = pd.concat([sample_dup_data, doc[doc["is_contain"] == '0']], axis=0)
    all_doc.sample(frac=1)
    print(path, all_doc.count())
    return all_doc

def is_contain_text_check(x):
    if x["query_text"] in x["doc_text"] or x["doc_text"] in x["query_text"]:
        return '1'
    else:
        return '0'


if __name__ == '__main__':
    doc_mt = load_sigle_df("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/train_1222-1230_total.csv",False)
    # doc_wa_poi = load_sigle_df("../../data/train_wm_poi.csv")
    # doc_wa_poi["geohash_doc"] = ""
    # doc_wa_spu = load_sigle_df("../../data/train_wm_spu.csv")
    # doc_wa_spu["geohash_doc"] = ""
    # doc_mt_deal = load_sigle_df("../../data/train_mt_deal.csv")
    # doc_mt_deal["geohash_doc"] = ""
    doc = doc_mt
    # doc = pd.concat([doc_wa_poi, doc_mt,doc_mt_deal,doc_wa_spu], axis=0)
    doc.sample(frac=1)
    data_dict = {}
    fr_json = codecs.open("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/tran_poi_data_1222-1230_total.json", "w", encoding="utf-8")
    neg_list = set()
    print("load data ........")
    for _ in range(len(doc)):
        _d = doc[_:_ + 1]
        key = _d["query_text"].values[0].strip() + "[SEP]" + _d["query_geohash"].values[0]
        if key in data_dict:
            data_dict[key].append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
        else:
            data_dict[key] = [_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0]]
        if _ % 10000 == 0:
            print("load ", _)
    print("struct neg data")
    index = 0
    for key, value in data_dict.items():
        query = key
        pos = value
        neg = []
        random_seed = random.randint(4, doc.shape[0])
        _neg_d = doc[random_seed - 3:random_seed]
        for _ in range(len(_neg_d)):
            _d = _neg_d[_:_ + 1]
            key_neg = _d["query_text"].values[0] + "[SEP]" + _d["query_geohash"].values[0]
            if key != key_neg:
                neg.append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
        data_json = json.dumps({"query": query, "pos": pos, "neg": neg}, ensure_ascii=False)
        fr_json.write(data_json + "\n")
        index += 1
        if index % 100000 == 0:
            print("neg_data ", index)
    fr_json.close()
```

其中主要使用的函数包括：

```python
import pandas as pd
doc = pd.read_csv(path, on_bad_lines='skip')
doc = doc.dropna(subset=["doc_text"])
doc.fillna('NULL', inplace=True)
sample_dup_data = doc[doc["is_contain"] == '1'].sample(frac=0.3)
all_doc = pd.concat([sample_dup_data, doc[doc["is_contain"] == '0']], axis=0)
all_doc.sample(frac=1)
data_dict = {}
fr_json = codecs.open("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/tran_poi_data_1222-1230_total.json", "w", encoding="utf-8")
neg_list = set()
for _ in range(len(doc)):
    _d = doc[_:_ + 1]
    key = _d["query_text"].values[0].strip() + "[SEP]" + _d["query_geohash"].values[0]
    if key in data_dict:
        data_dict[key].append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
    else:
        data_dict[key] = [_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0]]
for key, value in data_dict.items():
    query = key
    pos = value
    neg = []
    random_seed = random.randint(4, doc.shape[0])
    _neg_d = doc[random_seed - 3:random_seed]
    for _ in range(len(_neg_d)):
        _d = _neg_d[_:_ + 1]
        key_neg = _d["query_text"].values[0] + "[SEP]" + _d["query_geohash"].values[0]
        if key != key_neg:
            neg.append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
    data_json = json.dumps({"query": query, "pos": pos, "neg": neg}, ensure_ascii=False)
    fr_json.write(data_json + "\n")
    index += 1
    if index % 100000 == 0:
        print("neg_data ", index)
fr_json.close()
```

数据处理为上面的形式以后就需要参考paddle飞桨公开教程，从json格式的数据读取为numpy格式的数据。