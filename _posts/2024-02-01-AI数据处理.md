---
title: AI数据处理
date: 2024-02-01 16:09:00 +0800
categories: [tools, notes]
tags: [tools]     # TAG names should always be lowercase
math: true
mermaid: true
img_path: /commons/2024-02-01-数据处理/
author: hupenghui
---

> 进行人工智能相关开发，很重要的一点就是数据的准备与处理，而现实中的数据处理难点主要有两个：第一，数据并不全是以数字的形式存在的，很多的是以文字和图片的形式存在的，我们需要对这些计算机识别不了的数据手动转化为数字（图片还好说，读进来就是数字形式，主要是文字难以处理）。第二，现实中大部分业务数据都是存储在数据库中，需要我们使用sql指令从数据库中读取数据，然后才能使用python相关编程语言进行处理（使用sql对表数据进行处理，使用pandas对表格数据处理，使用numpy对数字数据处理）。因此第一部分介绍如何从数据库中读取数据。第二部分介绍如何对数据格式和内容进行转换。

## 数据准备的要点

自然语言处理中，大部分训练数据都是以json的形式构造的。json的结构很简单，就是字典的形式，有键有值。但是关键是：键应该怎么写？值应该怎么构造？多个样本数据怎么放在一起？这些才是关键，不要因为一点熟悉的知识就忘记了其他重要的知识。存储还是按照文本形式存储的，换行记得加，然后每一行都是一个json库dump下的字典。

从数据库中提取数据关键操作是把多个表链接起来，通过关键字来实现多个表之间的关联，可能跟我们最终需要的数据格式并不一致，但是一定包含我们所需要的所有数据。

然后通过pandas对数据格式进行调整，转存为json格式。

然后再从json格式转化为numpy数组格式，这个步骤往往是在继承datasets类的过程中实现的，即继承datasets类并实现数据格式的转换。然后我们可以得到具有标准形式的数据集，然后再通过实现dataloader类来把数据加载进dataloader。dataloader往往需要跟enumerate配合使用，这样我们可以逐批次的使用训练数据。参考paddle飞桨手册。

## 从数据库中读取数据

首先是从数据库中提取多个表的通过关键字链接起来的数据：参考美团实习经历

```sql
SELECT query_text,
       doc_text,
       query_geohash,
       geohash_doc,
       is_geo_query,
       CASE WHEN doc_text LIKE concat('%',query_text,'%') THEN 1
            ELSE 0
             END AS is_contain
  FROM (
        SELECT *
          FROM (
                SELECT query_text,
                       doc_text,
                       query_geohash,
                       geohash_doc,
                       is_geo_query,
                       row_number () OVER(PARTITION BY query_text ORDER BY rand()) AS rn
                  FROM (
                        SELECT DISTINCT qm.global_id AS global_id,
                               qm.query_text AS query_text,
                               qm.doc_text AS doc_text,
                               qm.geohash_doc AS geohash_doc,
                               qm.query_geohash AS query_geohash,
                               c_7.is_geo_query AS is_geo_query
                          FROM (
                                SELECT log.search_keyword AS query_text,
                                       poi_dim.poi_name AS doc_text,
                                       poi_dim.geohash AS geohash_doc,
                                       log.geohash AS query_geohash,
                                       log.global_id AS global_id,log.partition_date as partition_date
                                  FROM (
                                        SELECT item_id,
                                               search_keyword,
                                               geohash,
                                               partition_date,
                                               global_id
                                          FROM mart_ssai.topic_search_result_item_funnel_d_d
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND item_id_system='mtPoi'
                                           AND valid_click_cnt >1
                                           AND search_type='mt_home_search'
                                       ) AS log
                                  JOIN (
                                        SELECT *
                                          FROM mart_ssai.dim_poi_common
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND poi_name <> 'NULL'
                                       ) AS poi_dim
                                    ON log.item_id=poi_dim.poi_id
                                   AND log.partition_date=poi_dim.partition_date
                                 WHERE log.search_keyword <> poi_dim.poi_name
                               ) qm
                          JOIN (
                                SELECT global_id AS global_id,dt,
                                       CASE WHEN log LIKE "%loction_info:%" THEN 1
                                            ELSE 0
                                             END AS is_geo_query
                                  FROM log.dataapp_search_query_analysis
                                 WHERE dt between '20231123' and '20231127'
                                   AND SOURCE = 'com.sankuai.mobile.search.mergeservice'
                                   AND UUID != ''
                                   AND length(query) > 0
                               ) c_7
                            ON c_7.global_id = qm.global_id
                       ) c_8
               ) tmp
         WHERE rn <=5
       ) s3
```

上面提取出的数据只会在sql中存在，我们需要转存到文件中，就需要使用python。而使用python进行大规模数据处理就需要使用pyspark,需要创建一个sparksession，用它来读取数据。

```python
from pyspark import SparkConf
from pyspark.sql import SparkSession
sparkConf = SparkConf() \
    .setAppName("Jupyter") \
    .setMaster("yarn") \
    .set('spark.yarn.dist.archives', 'viewfs://hadoop-meituan/user/hadoop-mlp-jupyter/mtjupyter/archives/scipy-notebook-f8ea9eb.zip#ARCHIVE') \
    .set("spark.yarn.archive", "viewfs://hadoop-meituan/user/hadoop-hdp/notebook/spark-jars/spark_libs.zip") \
    .set("spark.yarn.queue", "root.zw03.hadoop-search.offline") \
    .set("spark.shuffle.service.enabled", "true") \
    .set("spark.dynamicAllocation.enabled", "true") \
    .set("spark.dynamicAllocation.minExecutors", "10") \
    .set("spark.dynamicAllocation.maxExecutors", "300") \
    .set("spark.executor.memory","4G") \
    .set("spark.driver.memory","7G") \
    .set("spark.input.dir.recursive","true") \
    .set("spark.sql.ignore.existed.function.enable","true") \
    .set("spark.yarn.maxAppAttempts", "1") \
    .set("spark.noResourceMaxInterval","86400s") \
    .set("spark.scheduler.mode", "FAIR") \
    .set("spark.executor.cores","2") \
    .set("spark.yarn.executor.memoryOverhead","1024") \
    .set("spark.yarn.driver.memoryOverhead","1024") \
    .set('spark.hadoop.hive.mt.renew.token.enable', 'true') \
    .set('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')
spark = SparkSession.builder.config(conf=sparkConf).enableHiveSupport().getOrCreate()
from pyspark.sql import HiveContext
sqlCtx = HiveContext(spark)
df = sqlCtx.sql("""SELECT query_text,
       doc_text,
       query_geohash,
       geohash_doc,
       is_geo_query,
       CASE WHEN doc_text LIKE concat('%',query_text,'%') THEN 1
            ELSE 0
             END AS is_contain
  FROM (
        SELECT *
          FROM (
                SELECT query_text,
                       doc_text,
                       query_geohash,
                       geohash_doc,
                       is_geo_query,
                       row_number () OVER(PARTITION BY query_text ORDER BY rand()) AS rn
                  FROM (
                        SELECT DISTINCT qm.global_id AS global_id,
                               qm.query_text AS query_text,
                               qm.doc_text AS doc_text,
                               qm.geohash_doc AS geohash_doc,
                               qm.query_geohash AS query_geohash,
                               c_7.is_geo_query AS is_geo_query
                          FROM (
                                SELECT log.search_keyword AS query_text,
                                       poi_dim.poi_name AS doc_text,
                                       poi_dim.geohash AS geohash_doc,
                                       log.geohash AS query_geohash,
                                       log.global_id AS global_id,log.partition_date as partition_date
                                  FROM (
                                        SELECT item_id,
                                               search_keyword,
                                               geohash,
                                               partition_date,
                                               global_id
                                          FROM mart_ssai.topic_search_result_item_funnel_d_d
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND item_id_system='mtPoi'
                                           AND valid_click_cnt >1
                                           AND search_type='mt_home_search'
                                       ) AS log
                                  JOIN (
                                        SELECT *
                                          FROM mart_ssai.dim_poi_common
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND poi_name <> 'NULL'
                                       )AS poi_dim
                                    ON log.item_id=poi_dim.poi_id
                                   AND log.partition_date=poi_dim.partition_date
                                 WHERE log.search_keyword <> poi_dim.poi_name
                               ) qm
                          JOIN (
                                SELECT global_id AS global_id,dt,
                                       CASE WHEN log LIKE "%loction_info:%" THEN 1
                                            ELSE 0
                                             END AS is_geo_query
                                  FROM log.dataapp_search_query_analysis
                                 WHERE dt between '20231123' and '20231127'
                                   AND SOURCE = 'com.sankuai.mobile.search.mergeservice'
                                   AND UUID != ''
                                   AND length(query) > 0
                               ) c_7
                            ON c_7.global_id = qm.global_id
                       ) c_8
               ) tmp
         WHERE rn <=5
       ) s3""")
# 存储数据
df.write.mode("overwrite").options(header="true").csv("viewfs://hadoop-meituan/user/hadoop-search/qikena/train_mt_poi_1123-1127")
```

可以将上述数据以csv的形式存储下来。这个select语句会创建一个表格数据，表头就是上面的属性名字，内容就是表的内容。当把它存下来时，格式就是原来的表格形式，不过是中间的分隔符用逗号来代替。
也就是说原来是：

| query_text | doc_text | query_geohash | geohash_doc | is_geo_query |
|:-----------|:---------|:--------------|:------------|-------------:|
| query      | doc      | query_hash    | doc_hash    | bool         |

然后存储下来就是：`query_text,doc_text,query_geohash,geohash_doc,is_geo_query\nquery,doc,query_hash,doc_hash,bool`。当然可能还需要进行其他处理，比如把spark的多个文件合并。通过hadoop合并多个带有表头的csv文件，要想不复制表头，需要按照下列命令进行。

```bash
#查看文件架构
!hdfs dfs -ls -h viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230
#删除同名文件
!hdfs dfs -ls -h viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
!hdfs dfs -rm -r viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
#复制文件
!hdfs dfs -cp viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/part-00000-05587d70-c041-4775-a226-a67646a67d35-c000.csv viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
#扩充文件
!hdfs dfs -ls viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/part-*.csv | grep -v 'part-00000-05587d70-c041-4775-a226-a67646a67d35-c000.csv' | awk '{print $NF}' | while read FILE; do hdfs dfs -cat "$FILE" | tail -n +2 | hdfs dfs -appendToFile - viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv;done
#拉取文件
!hdfs dfs -copyToLocal viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv /home/hadoop-search/dolphinfs_hdd_hadoop-search/hupenghui/data/train_1222-1230_total.csv
```

bash脚本中，普遍的循环操作的代码逻辑是：输出目录下的内容，选择每一行需要操作的文件名字，然后使用`while read`来读入文件名，然后在`do`里面定义操作，这样就可以对要修改的文件进行快捷修改。

当然也有可能表格中的内容是需要进一步进行分割等复杂处理的，可以使用这种方法，对其中一些内容进行分割，会返回列表，然后对列表可以取第一项（1）或其他项。也就是按照某一个特征分为多行，每一行的其余内容保持不变，只是会有新的一列`record`：

```sql
SELECT global_id,
       split(record, '|')[1] AS ner_name,
       split(record, '|')[2] AS ner_type
  FROM (
        SELECT partition_date,
               global_id,
               ner_info,
               record
          FROM (
                SELECT partition_date,
                       global_id,
                       get_json_object(ner_info, '$.bert_ner.path') AS ner_path,
                       ner_info
                  FROM mart_eif_flow.fact_log_stg_search_query_understand
                 WHERE partition_date = '2024-02-17'
                   AND req_id = 10
                   AND req_source = '["com.sankuai.mobile.search.mergeservice"]'
               ) LATERAL VIEW explode(split(ner_path, ',')) records AS record
       )
 WHERE length(record) > 0
```

通过`explode`可以把列表分为好几行。

验证集的构造：

```sql
## poi候选集
SELECT *
  FROM mart_ssai.dim_poi_common
 WHERE partition_date ='2023-12-03'
   AND poi_name <> 'NULL'
   AND city_name='北京'
   AND close_status=0
   AND mergedid=0
   AND poi_type = 0
## query-poi集合
SELECT item_id,
       search_keyword,
       geohash,
       partition_date,
       global_id
  FROM mart_ssai.topic_search_result_item_funnel_d_d
 WHERE partition_date ='2023-12-03'
   AND item_id_system='mtPoi'
   AND valid_click_cnt >=1
   AND search_type='mt_home_search'
   AND item_is_rcommend=0
   AND (locate_city_name='北京' OR page_city_name='北京')
```

## 查看数据的分布情况

在获取完数据后，往往需要对数据的各个属性的分布情况进行分析。常用的就是python，当然使用hadoop应该也可以进行分析。通过python使用hadoop进行大规模数据处理，就需要使用pyspark建立一个sparksession。

```python
#建立一个sparksession
from pyspark import SparkConf
from pyspark.sql import SparkSession
sparkConf = SparkConf() \
    .setAppName("Jupyter") \
    .setMaster("yarn") \
    .set('spark.yarn.dist.archives', 'viewfs://hadoop-meituan/user/hadoop-mlp-jupyter/mtjupyter/archives/scipy-notebook-f8ea9eb.zip#ARCHIVE') \
    .set("spark.yarn.archive", "viewfs://hadoop-meituan/user/hadoop-hdp/notebook/spark-jars/spark_libs.zip") \
    .set("spark.yarn.queue", "root.zw03.hadoop-search.offline") \
    .set("spark.shuffle.service.enabled", "true") \
    .set("spark.dynamicAllocation.enabled", "true") \
    .set("spark.dynamicAllocation.minExecutors", "10") \
    .set("spark.dynamicAllocation.maxExecutors", "300") \
    .set("spark.executor.memory","4G") \
    .set("spark.driver.memory","7G") \
    .set("spark.input.dir.recursive","true") \
    .set("spark.sql.ignore.existed.function.enable","true") \
    .set("spark.yarn.maxAppAttempts", "1") \
    .set("spark.noResourceMaxInterval","86400s") \
    .set("spark.scheduler.mode", "FAIR") \
    .set("spark.executor.cores","2") \
    .set("spark.yarn.executor.memoryOverhead","1024") \
    .set("spark.yarn.driver.memoryOverhead","1024") \
    .set('spark.hadoop.hive.mt.renew.token.enable', 'true') \
    .set('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')
spark = SparkSession.builder.config(conf=sparkConf).enableHiveSupport().getOrCreate()
# 读取HDFS上的文件，这里假设文件是CSV格式
df = spark.read.csv("viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv", header=True, inferSchema=True)
# 查看前五行文件
df.show(5)
# 选择不同的query_text并去重
layers = df.select("query_text").distinct()
# 用isNull()方法找到'partition_date'列中的空值
null_values = df.filter(df['partition_date'].isNull())
# 找到所有（列中包含空值的行）
from pyspark.sql.functions import col
# 查找任何一列包含空值的行
null_values_any = df.filter(
    col('column1').isNull() | 
    col('column2').isNull() |
    col('column3').isNull() 
    # 以此类推，为每一列添加条件
)
# 查找所有列都是空值的行
null_values_all = df.filter(
    col('column1').isNull() &
    col('column2').isNull() &
    col('column3').isNull() 
    # 以此类推，为每一列添加条件
)
# 展示结果
null_values.show()
# 计算每个partition_date的数量
layers_count = df.groupBy("partition_date").count()
layers_count.show()
# 计算数据集中的总数
total_count = df.count()
# 计算每个partition_date的比例
layer_proportions = layers_count.withColumn("proportion", layers_count["count"] / total_count)
layer_proportions.show()
# 合并两个数据帧
final_sample = sampled_data.unionAll(df)
# 停止SparkSession
spark.stop()
```

上面的代码可以根据个人需要调换顺序，一般来说检查数据分布的流程为：读取数据`read`，对某一列进行`group by`查看列特征的分布情况，然后查看包含特殊值的那一行数据是什么情况`filter`，然后使用下一节中介绍的对数据的处理办法进行处理。

也可以使用python读取数据进行分析，示例代码是在jupyter中进行的。

```python
import json
import codecs
import pandas as pd
import random
path="/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/train_1215-1230_total.csv"
doc = pd.read_csv(path, on_bad_lines='skip')
doc = doc.dropna(subset=["doc_text"])
doc = doc.dropna(subset=["query_text"])
doc.fillna('NULL', inplace=True)
doc.count()#输出表格内各列的数量
#对于这个对象可以按照表格的方式进行处理。
doc_0=doc[doc["is_contain"] == '0']
doc_0
doc_1=doc[doc["is_contain"] == '1']
print(doc_1.count(),doc_1)
doc_is_contain=doc[doc["is_contain"] == 'is_contain']
print(doc_is_contain.count(),doc_is_contain)
grouped_df = doc.groupby('is_contain')
for name, group in grouped_df:
    print(f"Group name: {name}")
    print(group)
```

通过上面的代码可以看出各个列的值的分布情况。这部分代码比较简略，根据自己的需要对照上面的hadoop代码再进行补充即可。

## 使用python对表格数据进行转存（csv-json）

然后是把上面的数据转存为json格式的数据，一般是使用pandas进行处理。在这一步，主要需要做的就是删除空值，然后将数据按照一定格式存储下来，方便下一步的numpy处理。参考美团实习经历

```python
# -* -coding:utf-8 -*-

import json
import codecs
import pandas as pd
import random


def load_sigle_df(path,is_need_contain=True):
    train_data_csv = pd.read_csv(path, on_bad_lines='skip')
    doc = train_data_csv
    # 删除空值
    doc = doc.dropna(subset=["doc_text"])
    doc = doc.dropna(subset=["query_text"])
    if is_need_contain:
        is_contain_check = doc.apply(lambda x: is_contain_text_check(x), axis=1)
        doc["is_contain"] = is_contain_check
        doc.to_csv("../../data/train/train_mt_poi_7_is_contain.csv")
    # 填充其他值
    doc.fillna('NULL', inplace=True)
    # 随机采样
    sample_dup_data = doc[doc["is_contain"] == '1'].sample(frac=0.3)
    #拼接两部分
    all_doc = pd.concat([sample_dup_data, doc[doc["is_contain"] == '0']], axis=0)
    #all_doc.sample(frac=1)
    #print(path, all_doc.count())
    return all_doc

def is_contain_text_check(x):
    if x["query_text"] in x["doc_text"] or x["doc_text"] in x["query_text"]:
        return '1'
    else:
        return '0'


if __name__ == '__main__':
    doc_mt = load_sigle_df("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/train_1222-1230_total.csv",True)
    doc = doc_mt
    doc.sample(frac=1)
    data_dict = {}
    fr_json = codecs.open("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/tran_poi_data_1222-1230_total.json", "w", encoding="utf-8")
    neg_list = set()
    print("load data ........")
    for _ in range(len(doc)):
        _d = doc[_:_ + 1]
        key = _d["query_text"].values[0].strip() + "[SEP]" + _d["query_geohash"].values[0]
        if key in data_dict:
            data_dict[key].append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
        else:
            data_dict[key] = [_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0]]
        if _ % 10000 == 0:
            print("load ", _)
    print("struct neg data")
    index = 0
    for key, value in data_dict.items():
        query = key
        pos = value
        neg = []
        random_seed = random.randint(4, doc.shape[0])
        _neg_d = doc[random_seed - 3:random_seed]
        for _ in range(len(_neg_d)):
            _d = _neg_d[_:_ + 1]
            key_neg = _d["query_text"].values[0] + "[SEP]" + _d["query_geohash"].values[0]
            if key != key_neg:
                neg.append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
        data_json = json.dumps({"query": query, "pos": pos, "neg": neg}, ensure_ascii=False)
        fr_json.write(data_json + "\n")
        index += 1
        if index % 100000 == 0:
            print("neg_data ", index)
    fr_json.close()
```

其中主要使用的函数包括：

```python
# 导入pandas包
import pandas as pd
# 读取文件
doc = pd.read_csv(path, on_bad_lines='skip')
# 删除空行
doc = doc.dropna(subset=["doc_text"])
# 填充空行
doc.fillna('NULL', inplace=True)
# 根据某一列的某些特征选择数据并采样
sample_dup_data = doc[doc["is_contain"] == '1'].sample(frac=0.3)
all_doc = pd.concat([sample_dup_data, doc[doc["is_contain"] == '0']], axis=0)
all_doc.sample(frac=1)
#将数据存储到字典中
data_dict = {}
fr_json = codecs.open("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/tran_poi_data_1222-1230_total.json", "w", encoding="utf-8")
neg_list = set()
for _ in range(len(doc)):
    _d = doc[_:_ + 1]
    key = _d["query_text"].values[0].strip() + "[SEP]" + _d["query_geohash"].values[0]
    if key in data_dict:
        data_dict[key].append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
    else:
        data_dict[key] = [_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0]]
#字典的键是query的信息，值是doc的信息
#将字典中的数据转存为json标准形式的并存入文件中。
for key, value in data_dict.items():
    query = key
    pos = value
    neg = []
    random_seed = random.randint(4, doc.shape[0])
    _neg_d = doc[random_seed - 3:random_seed]
    for _ in range(len(_neg_d)):
        _d = _neg_d[_:_ + 1]
        key_neg = _d["query_text"].values[0] + "[SEP]" + _d["query_geohash"].values[0]
        if key != key_neg:
            neg.append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
    #合并为json格式
    data_json = json.dumps({"query": query, "pos": pos, "neg": neg}, ensure_ascii=False)
    #存入文件中
    fr_json.write(data_json + "\n")
    index += 1
    if index % 100000 == 0:
        print("neg_data ", index)
fr_json.close()
```

数据处理为上面的形式以后就需要参考paddle飞桨公开教程，从json格式的数据读取为numpy格式的数据。上面代码中的操作顺序也是可以根据个人需要调整的，一般的顺序就是：读取数据`read`，删除空行`dropna`,填充空行`fillna`,选择数据`[]`,根据需要将重复数据合并`{key:value}`（如果需要保留，就用字典存储，如果不需要保留就用python合并），将数据转存为json格式`json.dumps`，写入文件中`write`。

## 抛弃for循环使用dataframe

```python
import pandas
import json
import numpy as np
doc=pandas.read_csv("./eval_ner_nopart_address_brand_1231_beijing_total.csv")
print(len(doc['query_text']))
doc=doc.dropna(subset=['query_text'])
print(len(doc))
doc=doc.dropna(subset=['doc_text'])
print(len(doc))
def is_contain_text_check(x):
    if x["query_text"] in x["doc_text"] or x["doc_text"] in x["query_text"]:
        return '1'
    else:
        return '0'
doc['contain']=doc.apply(lambda x:is_contain_text_check(x),axis=1)
doc['query_hash']=doc['query_text']+"[SEP]"+doc['query_geohash']
doc['doc_hash']=doc['doc_text']+"[SEP]"+doc['geohash_doc']
#添加三个负样本列
# 使用numpy的random.permutation函数对列'A'的索引进行随机打乱
shuffled_index = np.random.permutation(doc.index)
print(len(shuffled_index))
print(min(shuffled_index))
print(doc['doc_hash'].sample(frac=1))
print(len(doc['doc_hash'].sample(frac=1)))
#添加三个负样本列

# 使用打乱的索引对列'A'进行重新赋值
doc['neg_1']=doc['doc_hash'].sample(frac=1).reset_index(drop=True)
# 使用numpy的random.permutation函数对列'A'的索引进行随机打乱
shuffled_index = np.random.permutation(doc.index)

# 使用打乱的索引对列'A'进行重新赋值
doc['neg_2']=doc['doc_hash'].sample(frac=1).reset_index(drop=True)
# 使用numpy的random.permutation函数对列'A'的索引进行随机打乱
shuffled_index = np.random.permutation(doc.index)

# 使用打乱的索引对列'A'进行重新赋值
doc['neg_3']=doc['doc_hash'].sample(frac=1).reset_index(drop=True)
#把三个列合并
doc['neg']=doc.apply(lambda row:[row['neg_1'],row['neg_2'],row['neg_3']],axis=1)
gathered = doc.groupby('query_hash').agg({'doc_hash':list,'neg':list}).reset_index()
gathered['neg_tot']=gathered['neg'].apply(lambda x: x[0])
print(gathered['neg'])
print(gathered['neg'].apply(lambda x: x[0]))
gathered=gathered.drop('neg',axis=1)
#生成负样本
print(gathered['neg_tot'][0])
```

## 使用python将数据转为数字形式（json-number）

参考：[使用协同过滤实现电影推荐](https://www.paddlepaddle.org.cn/documentation/docs/zh/practices/recommendations/collaborative_filtering.html)

对于数据进行处理，把数据中类别特征转化为数字特征：

```python
df = pd.read_csv('ml-latest-small/ratings.csv')
user_ids = df["userId"].unique().tolist()
user2user_encoded = {x: i for i, x in enumerate(user_ids)}
userencoded2user = {i: x for i, x in enumerate(user_ids)}
#生成映射字典
movie_ids = df["movieId"].unique().tolist()
movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}
movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}
#按照映射字典进行映射（map函数）
df["user"] = df["userId"].map(user2user_encoded)
df["movie"] = df["movieId"].map(movie2movie_encoded)
#用户名和电影名都变成了唯一的数字id
#将评分定义为float类型
num_users = len(user2user_encoded)
num_movies = len(movie_encoded2movie)
df["rating"] = df["rating"].values.astype(np.float32)
# 最小和最大额定值将在以后用于标准化额定值
min_rating = min(df["rating"])
max_rating = max(df["rating"])
df = df.sample(frac=1, random_state=42)
x = df[["user", "movie"]].values
# 规范化0和1之间的目标。使训练更容易。
y = df["rating"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values
# 上面这个分的有点早了，不过应该不会改变顺序。
# 分割之后不能再sample了
# 假设对90%的数据进行训练，对10%的数据进行验证。
train_indices = int(0.9 * df.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:],
)
y_train = y_train[: ,np.newaxis]
y_val = y_val[: ,np.newaxis]
y_train = y_train.astype(np.float32)
y_val = y_val.astype(np.float32)

# 自定义数据集
#映射式(map-style)数据集需要继承paddle.io.Dataset
#训练数据的准备分为两步，第一步是定义dataset子类，第二步定义dataloader子类。
#定义dataset子类，需要实现getitem，len方法，如果是流式数据，则需要定义一个迭代器，yield函数
class SelfDefinedDataset(Dataset):
    def __init__(self, data_x, data_y, mode = 'train'):
        super().__init__()
        self.data_x = data_x
        self.data_y = data_y
        self.mode = mode

    def __getitem__(self, idx):
        if self.mode == 'predict':
           return self.data_x[idx]
        else:
           return self.data_x[idx], self.data_y[idx]

    def __len__(self):
        return len(self.data_x)
        
traindataset = SelfDefinedDataset(x_train, y_train)
for data, label in traindataset:
    print(data.shape, label.shape)
    print(data, label)
    break
train_loader = paddle.io.DataLoader(traindataset, batch_size = 128, shuffle = True)
for batch_id, data in enumerate(train_loader):
    x_data = data[0]
    y_data = data[1]

    print(x_data.shape)
    print(y_data.shape)
    break

testdataset = SelfDefinedDataset(x_val, y_val)
test_loader = paddle.io.DataLoader(testdataset, batch_size = 128, shuffle = True)        
for batch_id, data in enumerate(test_loader()):
    x_data = data[0]
    y_data = data[1]

    print(x_data.shape)
    print(y_data.shape)
    break
#定义网络计算样本之间的相似性
EMBEDDING_SIZE = 50

class RecommenderNet(nn.Layer):
    def __init__(self, num_users, num_movies, embedding_size):
        super().__init__()
        self.num_users = num_users
        self.num_movies = num_movies
        self.embedding_size = embedding_size
        weight_attr_user = paddle.ParamAttr(
            regularizer = paddle.regularizer.L2Decay(1e-6),
            initializer = nn.initializer.KaimingNormal()
            )
        self.user_embedding = nn.Embedding(
            num_users,
            embedding_size,
            weight_attr=weight_attr_user
        )
        self.user_bias = nn.Embedding(num_users, 1)
        weight_attr_movie = paddle.ParamAttr(
            regularizer = paddle.regularizer.L2Decay(1e-6),
            initializer = nn.initializer.KaimingNormal()
            )
        self.movie_embedding = nn.Embedding(
            num_movies,
            embedding_size,
            weight_attr=weight_attr_movie
        )
        self.movie_bias = nn.Embedding(num_movies, 1)

    def forward(self, inputs):
        user_vector = self.user_embedding(inputs[:, 0])
        user_bias = self.user_bias(inputs[:, 0])
        movie_vector = self.movie_embedding(inputs[:, 1])
        movie_bias = self.movie_bias(inputs[:, 1])
        dot_user_movie = paddle.dot(user_vector, movie_vector)
        x = dot_user_movie + user_bias + movie_bias
        x = nn.functional.sigmoid(x)

        return x
#训练模型
model = RecommenderNet(num_users, num_movies, EMBEDDING_SIZE)
model = paddle.Model(model)

optimizer = paddle.optimizer.Adam(parameters=model.parameters(), learning_rate=0.0003)
loss = nn.BCELoss()
metric = paddle.metric.Accuracy()

# 设置visualdl路径
log_dir = './visualdl'
callback = paddle.callbacks.VisualDL(log_dir=log_dir)

model.prepare(optimizer, loss, metric)
model.fit(train_loader, epochs=5, save_dir='./checkpoints', verbose=1, callbacks=callback)
#模型评估
model.evaluate(test_loader, batch_size=64, verbose=1)
#模型预测
movie_df = pd.read_csv('ml-latest-small/movies.csv')

# 获取一个用户，查看他的推荐电影
user_id = df.userId.sample(1).iloc[0]
movies_watched_by_user = df[df.userId == user_id]
movies_not_watched = movie_df[
    ~movie_df["movieId"].isin(movies_watched_by_user.movieId.values)
]["movieId"]
movies_not_watched = list(
    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))
)
movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]
user_encoder = user2user_encoded.get(user_id)
user_movie_array = np.hstack(
    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)
)
testdataset = SelfDefinedDataset(user_movie_array, user_movie_array, mode = 'predict')
test_loader = paddle.io.DataLoader(testdataset, batch_size = 9703, shuffle = False, return_list=True,)   

ratings = model.predict(test_loader)
ratings = np.array(ratings)
ratings = np.squeeze(ratings, 0)
ratings = np.squeeze(ratings, 2)
ratings = np.squeeze(ratings, 0)
top_ratings_indices = ratings.argsort()[::-1][0:10]

print(top_ratings_indices)
recommended_movie_ids = [
    movie_encoded2movie.get(movies_not_watched[x][0]) for x in top_ratings_indices
]

print("用户的ID为: {}".format(user_id))
print("====" * 8)
print("用户评分较高的电影：")
print("----" * 8)
top_movies_user = (
    movies_watched_by_user.sort_values(by="rating", ascending=False)
    .head(5)
    .movieId.values
)
movie_df_rows = movie_df[movie_df["movieId"].isin(top_movies_user)]
for row in movie_df_rows.itertuples():
    print(row.title, ":", row.genres)

print("----" * 8)
print("为用户推荐的10部电影：")
print("----" * 8)
recommended_movies = movie_df[movie_df["movieId"].isin(recommended_movie_ids)]
for row in recommended_movies.itertuples():
    print(row.title, ":", row.genres)
```

这个代码还是不够全，不过也相对来说是这样了，对于类别数据，就是通过确定总数量然后再做embedding，对于非类别数据就是直接使用。
参考：[用N-Gram模型在莎士比亚文集中训练word embedding](https://www.paddlepaddle.org.cn/documentation/docs/zh/practices/nlp/n_gram_model.html)

在上面的自然语言处理任务重就做了词的嵌入，也是一样的处理办法，不过需要考虑词的频率。首先把特殊字符替换为空格字符，然后按空格分割单词，并统计词频，选出前2500个词频最高的词作为词库，然后再把其余的字符都替换为空字符。然后构造训练数据，然后构造`datasets`子类，并定义`dataloader`，然后定义一个将`id`映射为长度为词库大小的向量作为预测分布，然后损失函数使用交叉熵，我们就可以得到这个模型。
