---
title: AI数据处理
date: 2024-02-01 16:09:00 +0800
categories: [tools, notes]
tags: [tools]     # TAG names should always be lowercase
math: true
mermaid: true
img_path: /commons/2024-02-01-数据处理/
author: hupenghui
---

> 进行人工智能相关开发，很重要的一点就是数据的准备与处理，而现实中的数据处理难点主要有两个：第一，数据并不全是以数字的形式存在的，很多的是以文字和图片的形式存在的，我们需要对这些计算机识别不了的数据手动转化为数字（图片还好说，读进来就是数字形式，主要是文字难以处理）。第二，现实中大部分业务数据都是存储在数据库中，需要我们使用sql指令从数据库中读取数据，然后才能使用python相关编程语言进行处理（使用sql对表数据进行处理，使用pandas对表格数据处理，使用numpy对数字数据处理）。因此第一部分介绍如何从数据库中读取数据。第二部分介绍如何对数据格式和内容进行转换。

## 数据准备的要点

自然语言处理中，大部分训练数据都是以json的形式构造的。json的结构很简单，就是字典的形式，有键有值。但是关键是：键应该怎么写？值应该怎么构造？多个样本数据怎么放在一起？这些才是关键，不要因为一点熟悉的知识就忘记了其他重要的知识。存储还是按照文本形式存储的，换行记得加，然后每一行都是一个json库dump下的字典。

从数据库中提取数据关键操作是把多个表链接起来，通过关键字来实现多个表之间的关联，可能跟我们最终需要的数据格式并不一致，但是一定包含我们所需要的所有数据。

然后通过pandas对数据格式进行调整，转存为json格式。

然后再从json格式转化为numpy数组格式，这个步骤往往是在继承datasets类的过程中实现的，即继承datasets类并实现数据格式的转换。然后我们可以得到具有标准形式的数据集，然后再通过实现dataloader类来把数据加载进dataloader。dataloader往往需要跟enumerate配合使用，这样我们可以逐批次的使用训练数据。参考paddle飞桨手册。

## 从数据库中读取数据

首先是从数据库中提取多个表的通过关键字链接起来的数据：参考美团实习经历

```sql
SELECT query_text,
       doc_text,
       query_geohash,
       geohash_doc,
       is_geo_query,
       CASE WHEN doc_text LIKE concat('%',query_text,'%') THEN 1
            ELSE 0
             END AS is_contain
  FROM (
        SELECT *
          FROM (
                SELECT query_text,
                       doc_text,
                       query_geohash,
                       geohash_doc,
                       is_geo_query,
                       row_number () OVER(PARTITION BY query_text ORDER BY rand()) AS rn
                  FROM (
                        SELECT DISTINCT qm.global_id AS global_id,
                               qm.query_text AS query_text,
                               qm.doc_text AS doc_text,
                               qm.geohash_doc AS geohash_doc,
                               qm.query_geohash AS query_geohash,
                               c_7.is_geo_query AS is_geo_query
                          FROM (
                                SELECT log.search_keyword AS query_text,
                                       poi_dim.poi_name AS doc_text,
                                       poi_dim.geohash AS geohash_doc,
                                       log.geohash AS query_geohash,
                                       log.global_id AS global_id,log.partition_date as partition_date
                                  FROM (
                                        SELECT item_id,
                                               search_keyword,
                                               geohash,
                                               partition_date,
                                               global_id
                                          FROM mart_ssai.topic_search_result_item_funnel_d_d
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND item_id_system='mtPoi'
                                           AND valid_click_cnt >1
                                           AND search_type='mt_home_search'
                                       ) AS log
                                  JOIN (
                                        SELECT *
                                          FROM mart_ssai.dim_poi_common
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND poi_name <> 'NULL'
                                       ) AS poi_dim
                                    ON log.item_id=poi_dim.poi_id
                                   AND log.partition_date=poi_dim.partition_date
                                 WHERE log.search_keyword <> poi_dim.poi_name
                               ) qm
                          JOIN (
                                SELECT global_id AS global_id,dt,
                                       CASE WHEN log LIKE "%loction_info:%" THEN 1
                                            ELSE 0
                                             END AS is_geo_query
                                  FROM log.dataapp_search_query_analysis
                                 WHERE dt between '20231123' and '20231127'
                                   AND SOURCE = 'com.sankuai.mobile.search.mergeservice'
                                   AND UUID != ''
                                   AND length(query) > 0
                               ) c_7
                            ON c_7.global_id = qm.global_id
                       ) c_8
               ) tmp
         WHERE rn <=5
       ) s3
```

上面提取出的数据只会在sql中存在，我们需要转存到文件中，就需要使用python。而使用python进行大规模数据处理就需要使用pyspark,需要创建一个sparksession，用它来读取数据。

```python
from pyspark import SparkConf
from pyspark.sql import SparkSession
sparkConf = SparkConf() \
    .setAppName("Jupyter") \
    .setMaster("yarn") \
    .set('spark.yarn.dist.archives', 'viewfs://hadoop-meituan/user/hadoop-mlp-jupyter/mtjupyter/archives/scipy-notebook-f8ea9eb.zip#ARCHIVE') \
    .set("spark.yarn.archive", "viewfs://hadoop-meituan/user/hadoop-hdp/notebook/spark-jars/spark_libs.zip") \
    .set("spark.yarn.queue", "root.zw03.hadoop-search.offline") \
    .set("spark.shuffle.service.enabled", "true") \
    .set("spark.dynamicAllocation.enabled", "true") \
    .set("spark.dynamicAllocation.minExecutors", "10") \
    .set("spark.dynamicAllocation.maxExecutors", "300") \
    .set("spark.executor.memory","4G") \
    .set("spark.driver.memory","7G") \
    .set("spark.input.dir.recursive","true") \
    .set("spark.sql.ignore.existed.function.enable","true") \
    .set("spark.yarn.maxAppAttempts", "1") \
    .set("spark.noResourceMaxInterval","86400s") \
    .set("spark.scheduler.mode", "FAIR") \
    .set("spark.executor.cores","2") \
    .set("spark.yarn.executor.memoryOverhead","1024") \
    .set("spark.yarn.driver.memoryOverhead","1024") \
    .set('spark.hadoop.hive.mt.renew.token.enable', 'true') \
    .set('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')
spark = SparkSession.builder.config(conf=sparkConf).enableHiveSupport().getOrCreate()
from pyspark.sql import HiveContext
sqlCtx = HiveContext(spark)
df = sqlCtx.sql("""SELECT query_text,
       doc_text,
       query_geohash,
       geohash_doc,
       is_geo_query,
       CASE WHEN doc_text LIKE concat('%',query_text,'%') THEN 1
            ELSE 0
             END AS is_contain
  FROM (
        SELECT *
          FROM (
                SELECT query_text,
                       doc_text,
                       query_geohash,
                       geohash_doc,
                       is_geo_query,
                       row_number () OVER(PARTITION BY query_text ORDER BY rand()) AS rn
                  FROM (
                        SELECT DISTINCT qm.global_id AS global_id,
                               qm.query_text AS query_text,
                               qm.doc_text AS doc_text,
                               qm.geohash_doc AS geohash_doc,
                               qm.query_geohash AS query_geohash,
                               c_7.is_geo_query AS is_geo_query
                          FROM (
                                SELECT log.search_keyword AS query_text,
                                       poi_dim.poi_name AS doc_text,
                                       poi_dim.geohash AS geohash_doc,
                                       log.geohash AS query_geohash,
                                       log.global_id AS global_id,log.partition_date as partition_date
                                  FROM (
                                        SELECT item_id,
                                               search_keyword,
                                               geohash,
                                               partition_date,
                                               global_id
                                          FROM mart_ssai.topic_search_result_item_funnel_d_d
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND item_id_system='mtPoi'
                                           AND valid_click_cnt >1
                                           AND search_type='mt_home_search'
                                       ) AS log
                                  JOIN (
                                        SELECT *
                                          FROM mart_ssai.dim_poi_common
                                         WHERE partition_date BETWEEN'2023-11-23' AND '2023-11-27'
                                           AND poi_name <> 'NULL'
                                       )AS poi_dim
                                    ON log.item_id=poi_dim.poi_id
                                   AND log.partition_date=poi_dim.partition_date
                                 WHERE log.search_keyword <> poi_dim.poi_name
                               ) qm
                          JOIN (
                                SELECT global_id AS global_id,dt,
                                       CASE WHEN log LIKE "%loction_info:%" THEN 1
                                            ELSE 0
                                             END AS is_geo_query
                                  FROM log.dataapp_search_query_analysis
                                 WHERE dt between '20231123' and '20231127'
                                   AND SOURCE = 'com.sankuai.mobile.search.mergeservice'
                                   AND UUID != ''
                                   AND length(query) > 0
                               ) c_7
                            ON c_7.global_id = qm.global_id
                       ) c_8
               ) tmp
         WHERE rn <=5
       ) s3""")
# 存储数据
df.write.mode("overwrite").options(header="true").csv("viewfs://hadoop-meituan/user/hadoop-search/qikena/train_mt_poi_1123-1127")
```

可以将上述数据以csv的形式存储下来。这个select语句会创建一个表格数据，表头就是上面的属性名字，内容就是表的内容。当把它存下来时，格式就是原来的表格形式，不过是中间的分隔符用逗号来代替。
也就是说原来是：

| query_text | doc_text | query_geohash | geohash_doc | is_geo_query |
|:-----------|:---------|:--------------|:------------|-------------:|
| query      | doc      | query_hash    | doc_hash    | bool         |

然后存储下来就是：`query_text,doc_text,query_geohash,geohash_doc,is_geo_query\nquery,doc,query_hash,doc_hash,bool`。当然可能还需要进行其他处理，比如把spark的多个文件合并。通过hadoop合并多个带有表头的csv文件，需要按照下列命令进行。

```bash
#查看文件架构
!hdfs dfs -ls -h viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230
#删除同名文件
!hdfs dfs -ls -h viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
!hdfs dfs -rm -r viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
#复制文件
!hdfs dfs -cp viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/part-00000-05587d70-c041-4775-a226-a67646a67d35-c000.csv viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv
#扩充文件
!hdfs dfs -ls viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/part-*.csv | grep -v 'part-00000-05587d70-c041-4775-a226-a67646a67d35-c000.csv' | awk '{print $NF}' | while read FILE; do hdfs dfs -cat "$FILE" | tail -n +2 | hdfs dfs -appendToFile - viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv;done
#拉取文件
!hdfs dfs -copyToLocal viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv /home/hadoop-search/dolphinfs_hdd_hadoop-search/hupenghui/data/train_1222-1230_total.csv
```

bash脚本中，普遍的循环操作的代码逻辑是：输出目录下的内容，选择每一行需要操作的文件名字，然后使用`while read`来读入文件名，然后在`do`里面定义操作，这样就可以对要修改的文件进行快捷修改。

当然也有可能表格中的内容是需要进一步进行分割等复杂处理的，可以使用这种方法：

```sql
SELECT global_id,
       split(record, '|')[1] AS ner_name,
       split(record, '|')[2] AS ner_type
  FROM (
        SELECT partition_date,
               global_id,
               ner_info,
               record
          FROM (
                SELECT partition_date,
                       global_id,
                       get_json_object(ner_info, '$.bert_ner.path') AS ner_path,
                       ner_info
                  FROM mart_eif_flow.fact_log_stg_search_query_understand
                 WHERE partition_date = '2024-02-17'
                   AND req_id = 10
                   AND req_source = '["com.sankuai.mobile.search.mergeservice"]'
               ) LATERAL VIEW explode(split(ner_path, ',')) records AS record
       )
 WHERE length(record) > 0
```

## 查看数据的分布情况

在获取完数据后，往往需要对数据的各个属性的分布情况进行分析。常用的就是python，当然使用hadoop应该也可以进行分析。通过python使用hadoop进行大规模数据处理，就需要使用pyspark建立一个sparksession。

```python
#建立一个sparksession
from pyspark import SparkConf
from pyspark.sql import SparkSession
sparkConf = SparkConf() \
    .setAppName("Jupyter") \
    .setMaster("yarn") \
    .set('spark.yarn.dist.archives', 'viewfs://hadoop-meituan/user/hadoop-mlp-jupyter/mtjupyter/archives/scipy-notebook-f8ea9eb.zip#ARCHIVE') \
    .set("spark.yarn.archive", "viewfs://hadoop-meituan/user/hadoop-hdp/notebook/spark-jars/spark_libs.zip") \
    .set("spark.yarn.queue", "root.zw03.hadoop-search.offline") \
    .set("spark.shuffle.service.enabled", "true") \
    .set("spark.dynamicAllocation.enabled", "true") \
    .set("spark.dynamicAllocation.minExecutors", "10") \
    .set("spark.dynamicAllocation.maxExecutors", "300") \
    .set("spark.executor.memory","4G") \
    .set("spark.driver.memory","7G") \
    .set("spark.input.dir.recursive","true") \
    .set("spark.sql.ignore.existed.function.enable","true") \
    .set("spark.yarn.maxAppAttempts", "1") \
    .set("spark.noResourceMaxInterval","86400s") \
    .set("spark.scheduler.mode", "FAIR") \
    .set("spark.executor.cores","2") \
    .set("spark.yarn.executor.memoryOverhead","1024") \
    .set("spark.yarn.driver.memoryOverhead","1024") \
    .set('spark.hadoop.hive.mt.renew.token.enable', 'true') \
    .set('spark.dynamicAllocation.cachedExecutorIdleTimeout', '1200s')
spark = SparkSession.builder.config(conf=sparkConf).enableHiveSupport().getOrCreate()
# 读取HDFS上的文件，这里假设文件是CSV格式
df = spark.read.csv("viewfs://hadoop-meituan/user/hadoop-search/hupenghui02/train_mt_poi_total_date_1222-1230/merged.csv", header=True, inferSchema=True)
# 查看前五行文件
df.show(5)
# 选择不同的query_text并去重
layers = df.select("query_text").distinct()
# 用isNull()方法找到'partition_date'列中的空值
null_values = df.filter(df['partition_date'].isNull())
# 找到所有（列中包含空值的行）
from pyspark.sql.functions import col
# 查找任何一列包含空值的行
null_values_any = df.filter(
    col('column1').isNull() | 
    col('column2').isNull() |
    col('column3').isNull() 
    # 以此类推，为每一列添加条件
)
# 查找所有列都是空值的行
null_values_all = df.filter(
    col('column1').isNull() &
    col('column2').isNull() &
    col('column3').isNull() 
    # 以此类推，为每一列添加条件
)
# 展示结果
null_values.show()
# 计算每个partition_date的数量
layers_count = df.groupBy("partition_date").count()
layers_count.show()
# 计算数据集中的总数
total_count = df.count()
# 计算每个partition_date的比例
layer_proportions = layers_count.withColumn("proportion", layers_count["count"] / total_count)
layer_proportions.show()
# 合并两个数据帧
final_sample = sampled_data.unionAll(df)
# 停止SparkSession
spark.stop()
```

上面的代码可以根据个人需要调换顺序，一般来说检查数据分布的流程为：读取数据`read`，对某一列进行`group by`查看列特征的分布情况，然后查看包含特殊值的那一行数据是什么情况`filter`，然后使用下一节中介绍的对数据的处理办法进行处理。

也可以使用python读取数据进行分析，示例代码是在jupyter中进行的。

```python
import json
import codecs
import pandas as pd
import random
path="/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/train_1215-1230_total.csv"
doc = pd.read_csv(path, on_bad_lines='skip')
doc = doc.dropna(subset=["doc_text"])
doc = doc.dropna(subset=["query_text"])
doc.fillna('NULL', inplace=True)
doc.count()#输出表格内各列的数量
#对于这个对象可以按照表格的方式进行处理。
doc_0=doc[doc["is_contain"] == '0']
doc_0
doc_1=doc[doc["is_contain"] == '1']
print(doc_1.count(),doc_1)
doc_is_contain=doc[doc["is_contain"] == 'is_contain']
print(doc_is_contain.count(),doc_is_contain)
grouped_df = doc.groupby('is_contain')
for name, group in grouped_df:
    print(f"Group name: {name}")
    print(group)
```

通过上面的代码可以看出各个列的值的分布情况。这部分代码比较简略，根据自己的需要对照上面的hadoop代码再进行补充即可。

## 使用python对表格数据进行转存（csv-json）

然后是把上面的数据转存为json格式的数据，一般是使用pandas进行处理。在这一步，主要需要做的就是删除空值，然后将数据按照一定格式存储下来，方便下一步的numpy处理。参考美团实习经历

```python
# -* -coding:utf-8 -*-

import json
import codecs
import pandas as pd
import random


def load_sigle_df(path,is_need_contain=True):
    train_data_csv = pd.read_csv(path, on_bad_lines='skip')
    doc = train_data_csv
    # 删除空值
    doc = doc.dropna(subset=["doc_text"])
    doc = doc.dropna(subset=["query_text"])
    if is_need_contain:
        is_contain_check = doc.apply(lambda x: is_contain_text_check(x), axis=1)
        doc["is_contain"] = is_contain_check
        doc.to_csv("../../data/train/train_mt_poi_7_is_contain.csv")
    # 填充其他值
    doc.fillna('NULL', inplace=True)
    # 随机采样
    sample_dup_data = doc[doc["is_contain"] == '1'].sample(frac=0.3)
    #拼接两部分
    all_doc = pd.concat([sample_dup_data, doc[doc["is_contain"] == '0']], axis=0)
    #all_doc.sample(frac=1)
    #print(path, all_doc.count())
    return all_doc

def is_contain_text_check(x):
    if x["query_text"] in x["doc_text"] or x["doc_text"] in x["query_text"]:
        return '1'
    else:
        return '0'


if __name__ == '__main__':
    doc_mt = load_sigle_df("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/train_1222-1230_total.csv",True)
    doc = doc_mt
    doc.sample(frac=1)
    data_dict = {}
    fr_json = codecs.open("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/tran_poi_data_1222-1230_total.json", "w", encoding="utf-8")
    neg_list = set()
    print("load data ........")
    for _ in range(len(doc)):
        _d = doc[_:_ + 1]
        key = _d["query_text"].values[0].strip() + "[SEP]" + _d["query_geohash"].values[0]
        if key in data_dict:
            data_dict[key].append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
        else:
            data_dict[key] = [_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0]]
        if _ % 10000 == 0:
            print("load ", _)
    print("struct neg data")
    index = 0
    for key, value in data_dict.items():
        query = key
        pos = value
        neg = []
        random_seed = random.randint(4, doc.shape[0])
        _neg_d = doc[random_seed - 3:random_seed]
        for _ in range(len(_neg_d)):
            _d = _neg_d[_:_ + 1]
            key_neg = _d["query_text"].values[0] + "[SEP]" + _d["query_geohash"].values[0]
            if key != key_neg:
                neg.append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
        data_json = json.dumps({"query": query, "pos": pos, "neg": neg}, ensure_ascii=False)
        fr_json.write(data_json + "\n")
        index += 1
        if index % 100000 == 0:
            print("neg_data ", index)
    fr_json.close()
```

其中主要使用的函数包括：

```python
# 导入pandas包
import pandas as pd
# 读取文件
doc = pd.read_csv(path, on_bad_lines='skip')
# 删除空行
doc = doc.dropna(subset=["doc_text"])
# 填充空行
doc.fillna('NULL', inplace=True)
# 根据某一列的某些特征选择数据并采样
sample_dup_data = doc[doc["is_contain"] == '1'].sample(frac=0.3)
all_doc = pd.concat([sample_dup_data, doc[doc["is_contain"] == '0']], axis=0)
all_doc.sample(frac=1)
#将数据存储到字典中
data_dict = {}
fr_json = codecs.open("/mnt/dolphinfs/hdd_pool/docker/user/hadoop-search/hupenghui/data/tran_poi_data_1222-1230_total.json", "w", encoding="utf-8")
neg_list = set()
for _ in range(len(doc)):
    _d = doc[_:_ + 1]
    key = _d["query_text"].values[0].strip() + "[SEP]" + _d["query_geohash"].values[0]
    if key in data_dict:
        data_dict[key].append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
    else:
        data_dict[key] = [_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0]]
#字典的键是query的信息，值是doc的信息
#将字典中的数据转存为json标准形式的并存入文件中。
for key, value in data_dict.items():
    query = key
    pos = value
    neg = []
    random_seed = random.randint(4, doc.shape[0])
    _neg_d = doc[random_seed - 3:random_seed]
    for _ in range(len(_neg_d)):
        _d = _neg_d[_:_ + 1]
        key_neg = _d["query_text"].values[0] + "[SEP]" + _d["query_geohash"].values[0]
        if key != key_neg:
            neg.append(_d["doc_text"].values[0] + "[SEP]" + _d["geohash_doc"].values[0])
    #合并为json格式
    data_json = json.dumps({"query": query, "pos": pos, "neg": neg}, ensure_ascii=False)
    #存入文件中
    fr_json.write(data_json + "\n")
    index += 1
    if index % 100000 == 0:
        print("neg_data ", index)
fr_json.close()
```

数据处理为上面的形式以后就需要参考paddle飞桨公开教程，从json格式的数据读取为numpy格式的数据。上面代码中的操作顺序也是可以根据个人需要调整的，一般的顺序就是：读取数据`read`，删除空行`dropna`,填充空行`fillna`,选择数据`[]`,根据需要将重复数据合并`{key:value}`（如果需要保留，就用字典存储，如果不需要保留就用python合并），将数据转存为json格式`json.dumps`，写入文件中`write`。

## 使用python将数据转为数字形式（json-number）
