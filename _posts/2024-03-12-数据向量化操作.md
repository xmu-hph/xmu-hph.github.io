---
title: 数据向量化操作
date: 2024-03-12 14:13:00 +0800
categories: [tools, notes]
tags: [tools]     # TAG names should always be lowercase
math: true
mermaid: true
img_path: /commons/2024-03-12-数据向量化操作/
author: hupenghui
---

> 使用pandas进行数据处理，是以`dataframe`的形式，除了逐行或者逐列来操作，有很多是可以使用向量化操作，即统一操作。

## 验证数据处理

```python
import pandas as pd
import json
import time

time_record=[]
time_record.append(time.time())
print("start time:{0}".format(time_record[-1]))

filepath="eval_ner_nopart_address_brand_1231_beijing_total_type.csv"
doc=pd.read_csv(filepath,on_bad_lines='skip')

#query_ner,query_text,doc_text,geohash_doc,query_geohash,partition_date,doc_ner,doc_address,global_id,query_cnt,doc_brand

print(doc.iloc[0:2])
print(len(doc))

print("read csv time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

#选出指定时间段内的数据
print(len(doc))

print("length count time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc=doc.dropna(subset=['query_text','doc_text'])

print("dropna two columns time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

print(len(doc))
print(doc.iloc[0:2])

print("length count time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

def is_contain_text_check(x):
    if x["query_text"] in x["doc_text"] or x["doc_text"] in x["query_text"]:
        return '1'
    else:
        return '0'
#query_ner,query_text,doc_text,geohash_doc,query_geohash,partition_date,doc_ner,doc_address,global_id,doc_brand
doc['contain']=doc.apply(lambda x:is_contain_text_check(x),axis=1)

print(doc.iloc[0:2])

print("contain cloumn add time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

#query_ner,query_text,doc_text,geohash_doc,query_geohash,partition_date,doc_ner,doc_address,global_id,doc_brand,contain
doc.fillna('NULL',inplace=True)

print("fillna  time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc_1=doc[doc['contain']=='1'].sample(frac=0.3).reset_index(drop=True)

print("sample 1 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc_0=doc[doc['contain']=='0'].reset_index(drop=True)

print("sample 0 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc=pd.concat([doc_1,doc_0],axis=0).reset_index(drop=True)

print(len(doc))
print(doc.iloc[0:2])

print("concat 1 and 0 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc['query']=doc['query_text'].str.strip()+"[SEP]"+doc['query_geohash'].str.strip()

print("construct qury time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc['pos']=doc['doc_text'].str.strip()+"[SEP]"+doc['geohash_doc'].str.strip()+"[SEP]"+doc['doc_ner'].str.strip()

print("construct pos time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

print(doc.iloc[0:2])

doc['global_id_str']=doc['global_id'].astype(str)
#'12', '14', '10', '12020103', '12020102')
#[12, 14, 10, 12020103, 12020102]
print(len(doc))
for key in [12, 14, 10, 12020103, 12020102]:
    print(key)
    doc_temp=doc[doc['query_cnt']==key].reset_index(drop=True)
    print(len(doc_temp))
    group=doc_temp.groupby('query').agg({'pos':list,'global_id_str':list}).reset_index()
    print(group.iloc[0:2])
    print(len(group))
    group['global_id']=group['global_id_str']
    group.drop('global_id_str',axis=1,inplace=True)
    print(group.iloc[0:2])
    #outputpath="eval_1231_query_th_doc_thn_total_cnt_{}.json".format(key)
    #group.to_json(outputpath,orient='records',lines=True,force_ascii=False)
    print("write to json time:{0}".format(time.time()-time_record[-1]))
    time_record.append(time.time())
    

#gathered = doc.groupby('query').agg({'pos':list,'global_id_str':list}).reset_index()

#print("group together time:{0}".format(time.time()-time_record[-1]))
#time_record.append(time.time())

#print(len(gathered))
#print(gathered.iloc[0:2])


#gathered['global_id']=gathered['global_id_str']
#print(gathered.iloc[0:2])
#生成负样本
#print(gathered['neg_tot'][0])
#存储到文件中

#gathered.drop('global_id_str',axis=1,inplace=True)
#print(gathered.iloc[0:2])

#outputpath='eval_1231_query_th_doc_thn_total.json'
#gathered.to_json(outputpath,orient='records',lines=True,force_ascii=False)
print("all done")
print(time_record)
```

## 训练数据处理

```python
import pandas as pd
import json
import time

time_record=[]
time_record.append(time.time())
print("start time:{0}".format(time_record[-1]))

filepath="train_ner_nopart_address_brand_1222-1231_total.csv"
doc=pd.read_csv(filepath,on_bad_lines='skip')

print(doc.iloc[0:2])
print(len(doc))

print("read csv time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

#mydata=['2023-12-30']
mydata=['2023-12-22','2023-12-23','2023-12-24','2023-12-25','2023-12-26','2023-12-27','2023-12-28','2023-12-29','2023-12-30']
doc=doc[doc['partition_date'].isin(mydata)].reset_index(drop=True)

print(len(doc))
print(doc.iloc[0:2])

print("date select time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

#选出指定时间段内的数据
print(len(doc))

print("length count time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc=doc.dropna(subset=['query_text','doc_text'])

print("dropna two columns time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

print(len(doc))
print(doc.iloc[0:2])

print("length count time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

def is_contain_text_check(x):
    if x["query_text"] in x["doc_text"] or x["doc_text"] in x["query_text"]:
        return '1'
    else:
        return '0'
#query_ner,query_text,doc_text,geohash_doc,query_geohash,partition_date,doc_ner,doc_address,doc_brand
doc['contain']=doc.apply(lambda x:is_contain_text_check(x),axis=1)

print(doc.iloc[0:2])
# 注意：这里假设'query_text'和'doc_text'都不包含正则表达式特殊字符

#contain_query = doc['doc_text'].str.contains(doc['query_text'])

#print("doc contain query time:{0}".format(time.time()-time_record[-1]))
#time_record.append(time.time())

#contain_doc = doc['query_text'].str.contains(doc['doc_text'])

#print("query contain doc time:{0}".format(time.time()-time_record[-1]))
#time_record.append(time.time())

# 由于str.contains()返回布尔值，我们可以使用astype将布尔值转换为字符串'1'或'0'
#doc['contain'] = (contain_query | contain_doc).astype(int).astype(str)

print("contain cloumn add time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

#query_ner,query_text,doc_text,geohash_doc,query_geohash,partition_date,doc_ner,doc_address,doc_brand,contain
doc.fillna('NULL',inplace=True)

print("fillna  time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc_1=doc[doc['contain']=='1'].sample(frac=0.3).reset_index(drop=True)

print("sample 1 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc_0=doc[doc['contain']=='0'].reset_index(drop=True)

print("sample 0 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc=pd.concat([doc_1,doc_0],axis=0).reset_index(drop=True)

print(len(doc))
print(doc.iloc[0:2])

print("concat 1 and 0 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc['query']=doc['query_text'].str.strip()+"[SEP]"+doc['query_geohash'].str.strip()

print("construct qury time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

doc['pos']=doc['doc_text'].str.strip()+"[SEP]"+doc['geohash_doc'].str.strip()+"[SEP]"+doc['doc_ner'].str.strip()

print("construct pos time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

print(doc.iloc[0:2])
#添加三个负样本列
# 使用打乱的索引对列'A'进行重新赋值
doc['neg_1']=doc['pos'].sample(frac=1).reset_index(drop=True)

print("add neg 1 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())
# 使用打乱的索引对列'A'进行重新赋值
doc['neg_2']=doc['pos'].sample(frac=1).reset_index(drop=True)

print("add neg 2 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())
# 使用打乱的索引对列'A'进行重新赋值
doc['neg_3']=doc['pos'].sample(frac=1).reset_index(drop=True)

print("add neg 3 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())
#把三个列合并
#doc['neg_tot']=doc.apply(lambda row:[row['neg_1'],row['neg_2'],row['neg_3']],axis=1)
doc['neg_tot'] = list(zip(doc['neg_1'], doc['neg_2'], doc['neg_3']))

print("list zip neg 123 time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

gathered = doc.groupby('query').agg({'pos':list,'neg_tot':list}).reset_index()

print("group together time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

print(len(gathered))
print(gathered.iloc[0:2])


#gathered['neg']=gathered['neg_tot'].apply(lambda x: x[0])
gathered['neg']=gathered['neg_tot'].str[0]

print(gathered.iloc[0:2])

print("gather neg time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

#print(gathered['neg'])
#print(gathered['neg'].apply(lambda x: x[0]))
gathered=gathered.drop('neg_tot',axis=1)

print("drop column time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

#生成负样本
#print(gathered['neg_tot'][0])
#存储到文件中

outputpath='tran_poi_data_1222-1230_query_th_doc_thn_total.json'
gathered.to_json(outputpath,orient='records',lines=True,force_ascii=False)

print("write to json time:{0}".format(time.time()-time_record[-1]))
time_record.append(time.time())

print("all done")
print(time_record)
```