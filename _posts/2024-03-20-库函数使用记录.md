---
title: 库函数使用记录
date: 2024-03-20 10:57:00 +0800
categories: [tools, notes]
tags: [tools]     # TAG names should always be lowercase
math: true
mermaid: true
img_path: /commons/2024-03-20-库函数使用记录/
author: hupenghui
---

> github中有很多优秀的代码库，但是不太知道怎么使用，把学习使用的过程记录下来。

## flagembedding

参考：[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding?tab=readme-ov-file)

从代码上看，这个库函数的使用都类似一个pipeline，输入都是文本形式，先经过一个`tokenizer`，再经过`model`求出输出。

## transformer库使用

一直想学习一下huggingface做的transformer类库函数的用法，总算找到机会了。
[transformer库使用](https://transformers.run/)，当然也可以看huggingface的官方介绍：[bge-large-zh](https://huggingface.co/BAAI/bge-large-zh?)，两者结合起来看更加清晰。

## 库函数总结

面向系统路径跳转切换：`os`,`pathlib`

面向文件增删改查：`shutil`

面向日志：`logging`

面向数据处理序列化：`json`,`pickle`

面向注释的有：`typing`,``

面向命令行参数：`argparse`

面向python编译器的环境：`sys`

面向装饰器的函数：`@`

面向抽象类：`abc`

面向读取文件：`pandas`

面向处理数据：`numpy`,`torch`,`tensorflow`

面向进度条：`tqdm`

## 写一个庞大代码

写一个庞大又完善的代码库到底需要哪些库函数？之前的代码编写经验都是惨不忍睹，写出来的代码不好维护，功能也不全，让人痛苦万分。究其原因，要么就是我不知道还有这种方便的库函数，要不就是我用的少不会用，只能用很简单的做法。

为了解决这个问题，我想出了一个办法：通过确定要用到的库函数及库函数的使用位置让我能够规范化的开发代码，而不是要么毫无头绪要么考虑不周。因此有了本文，我将确定各个代码库的使用位置，以及使用用法，方便我成功开发出一个完整的代码。

## 库函数具体使用技巧总结

定义常量参数，可以使用数据类来定义，或者常见数据结构，或者命令行配置。

## 代码总结

配置文件可以用json或者yaml或者argparse，其中json是字典形式，yaml是缩进形式，argparse是属性配置形式。还有dataclass类配置文件。
存的时候用的什么指令，读的时候往往就跟他很类似。读文件的时候查询：使用pandas读取这样子的文件时使用什么命令？有哪些参数？往往需要对命令的参数做一些调整。

## 重要经验

直接问处理什么数据表？用什么模型？目标是什么？

看代码总结：不管什么代码，无非三部分：加载数据、加载模型、训练模型。其中按照存储位置可以分为：从云端加载、从本地加载、从内存加载，从云端加载即先下载再使用，从本地加载即已下载导入使用，从内存加载即已经加载进内存中。从代码整体来看，无非就是按照顺序执行加载数据、加载模型、训练模型的逻辑。

关键在于代码的封装，这部分很有技巧，封装封的我看不懂代码。先把不封装代码写完，然后把各个部分抽出来放进函数就是封装了。

重要的一个知识点就是：pip安装的库函数，和他在github上的库代码结构是一摸一样的，通过pip安装比源码安装有更大的好处，因为通过pip安装可以在python中检索到，这就导致在任何地方都可以用官方的运行命令运行，哪怕使用`python -m`去测试他的模块也是可以的。而使用源码安装的，只能在源码所在的文件夹中使用`python -m`运行命令，在别的地方就不行了，所以推荐`pip`安装。

由于库函数的存在，使得编写代码更多的是库函数的形式，也就是使用封装函数的形式。在大模型领域，这个问题尤为常见。各种封装函数比如`pytorch`,`tensorflow`,`scikit-learn`等。

比如通过使用`flagembedding`库函数来加载模型：

```python
from FlagEmbedding import FlagReranker
reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=True) #use fp16 can speed up computing

score = reranker.compute_score(['query', 'passage'])
print(score)

scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])
print(scores)
```

而这就导致了一个问题，实现同样功能的加载数据、加载模型的函数可能在不同的库函数中都存在，所以导致一个代码有多种调库的方式实现。比如上面的代码也可以用`huggingface`的`transformers`库函数来实现加载模型：

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer, BatchEncoding, PreTrainedTokenizerFast

tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')
model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')
model.eval()

pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]
with torch.no_grad():
    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()
    print(scores)
```

这导致我对这个东西的运行迷迷糊糊的，又是`tokenizer`，又是`flagreranker`，不明白究竟这个代码是怎么运行的，尤其在没有弄明白代码的时候这个问题尤为严重。

在仔细观察以后发现，其实`flagreranker`也是调用的`transformers`的函数，只是又封装了一层。格外呆比。比如`FlagReranker`就对应着`autotokenizer`,`automodelforsequenceclassification`这行命令。

加载模型以后需要进行训练，也就是用模型对输入数据进行处理得到损失，利用损失对模型进行优化。封装可以分为小封装大封装，小封装就是各个模块单独封装，就比如上面对于加载模型的封装，大封装就是整个代码从加载数据到加载模型到训练模型都封装起来。比如：

```python
torchrun --nproc_per_node {number of gpus} \
-m FlagEmbedding.reranker.run \
--output_dir {path to save model} \
--model_name_or_path BAAI/bge-reranker-base \
--train_data ./toy_finetune_data.jsonl \
--learning_rate 6e-5 \
--fp16 \
--num_train_epochs 5 \
--per_device_train_batch_size {batch size; set 1 for toy data} \
--gradient_accumulation_steps 4 \
--dataloader_drop_last True \
--train_group_size 16 \
--max_len 512 \
--weight_decay 0.01 \
--logging_steps 10 
```

这就是一个大封装的代码。因为加载数据、加载模型、优化模型都在这个代码中完成了。其中加载模型部分我们已经分析过了，加载数据也无非就是使用数据类从本地路径中读入数据，关键在于如何训练模型。这部分我一直没仔细处理过。在这个代码中，训练模型使用了：

```python
_trainer_class = CETrainer
trainer = _trainer_class(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=GroupCollator(tokenizer),
    tokenizer=tokenizer
)
Path(training_args.output_dir).mkdir(parents=True, exist_ok=True)
trainer.train()
trainer.save_model()
```

使用了trainer类，封装了训练模型、保存模型的相关代码。本质上仍就是是调用`transfomers`库函数的功能。

## 重新总结一下

代码安装可以使用源码安装或者包管理器比如`pip`、`conda`安装，包管理器安装的好处就是任何地方都能调用，源码安装就是可以清晰的看到代码，进而可以调用各个函数，缺点就是只能在文件夹路径下使用。

加载数据可以使用封装好的`transformers`中的`dataset`类，加载数据的逻辑也无非就是，读取文件路径，处理数据，转化为数字输出。

加载模型也可以使用库函数，背后的逻辑也是顺序指令的形式，根据路径名，加载模型，

上面的加载分为本地加载、云端加载、内存加载，云端加载无非就是先下载再加载，本地加载就是下载好了直接读取文件加载，内存加载就是已经加载好了，可以进一步使用模型训练模型了。

训练模型也可以使用封装好的`trainer`类，包括训练模型，评估模型，保存模型这三个顺序逻辑。这部分不是一次的，可能是循环顺序逻辑。即循环多次执行训练评估和保存。

这样子回头看这个`reranker`的代码确实清晰多了。看[Finetune cross-encoder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker)最重要的收获就是明白了包管理器和源码安装之间的区别和共同点。尤其是pip的这个任何地方都可以看做把代码下载下来的特性也太有用了。

第二个知识点就是分清楚了全过程名词和分模块名词，比如说微调就是涵盖了加载数据和加载模型、训练模型的全过程，而处理数据只是对应了加载数据这个模块过程。目前相对来说能够熟练操作加载数据这个过程，还是使用的库函数，要是从c语言开始写肯定就很难。

第三点就是联想到我补全了那个node.js的代码也是一样的逻辑。并没有从c语言基础开始写，只是问`chatgpt`：在这个（node.js）编译环境中，要实现这个功能（获取在线链接中的内容，并读取第3行），该用什么函数？`chatgpt`给我提供了库函数及使用方法，我凭借经验进行了微调，最终得到了好的结果。

第四点内容就是直接跟label做运算的才是最终的损失，需要经过包含参数的函数转换才能跟label进行运算的都不是损失，这个含参数的函数应该也算做模型的一部分。直接跟label计算交叉熵或者最小二乘损失的才是预测结果。就算我们需要的是中间的特征层，也不影响我们用特征转换后的结果作为损失函数。
