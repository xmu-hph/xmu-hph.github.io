---
title: 库函数使用记录
date: 2024-03-20 10:57:00 +0800
categories: [tools, notes]
tags: [tools]     # TAG names should always be lowercase
math: true
mermaid: true
img_path: /commons/2024-03-20-库函数使用记录/
author: hupenghui
---

> github中有很多优秀的代码库，但是不太知道怎么使用，把学习使用的过程记录下来。

## flagembedding

参考：[FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding?tab=readme-ov-file)

从代码上看，这个库函数的使用都类似一个pipeline，输入都是文本形式，先经过一个`tokenizer`，再经过`model`求出输出。

## transformer库使用

一直想学习一下huggingface做的transformer类库函数的用法，总算找到机会了。
[transformer库使用](https://transformers.run/)，当然也可以看huggingface的官方介绍：[bge-large-zh](https://huggingface.co/BAAI/bge-large-zh?)，两者结合起来看更加清晰。

## 库函数总结

面向系统路径跳转切换：`os`,`pathlib`

面向文件增删改查：`shutil`

面向日志：`logging`

面向数据处理序列化：`json`,`pickle`

面向注释的有：`typing`,``

面向命令行参数：`argparse`

面向python编译器的环境：`sys`

面向装饰器的函数：`@`

面向抽象类：`abc`

面向读取文件：`pandas`

面向处理数据：`numpy`,`torch`,`tensorflow`

面向进度条：`tqdm`

## 写一个庞大代码

写一个庞大又完善的代码库到底需要哪些库函数？之前的代码编写经验都是惨不忍睹，写出来的代码不好维护，功能也不全，让人痛苦万分。究其原因，要么就是我不知道还有这种方便的库函数，要不就是我用的少不会用，只能用很简单的做法。

为了解决这个问题，我想出了一个办法：通过确定要用到的库函数及库函数的使用位置让我能够规范化的开发代码，而不是要么毫无头绪要么考虑不周。因此有了本文，我将确定各个代码库的使用位置，以及使用用法，方便我成功开发出一个完整的代码。

## 库函数具体使用技巧总结

定义常量参数，可以使用数据类来定义，或者常见数据结构，或者命令行配置。

## 代码总结

配置文件可以用json或者yaml或者argparse，其中json是字典形式，yaml是缩进形式，argparse是属性配置形式。还有dataclass类配置文件。
存的时候用的什么指令，读的时候往往就跟他很类似。读文件的时候查询：使用pandas读取这样子的文件时使用什么命令？有哪些参数？往往需要对命令的参数做一些调整。

## 重要经验

直接问处理什么数据表？用什么模型？目标是什么？

看代码总结：不管什么代码，无非三部分：加载数据、加载模型、训练模型。其中按照存储位置可以分为：从云端加载、从本地加载、从内存加载，从云端加载即先下载再使用，从本地加载即已下载导入使用，从内存加载即已经加载进内存中。从代码整体来看，无非就是按照顺序执行加载数据、加载模型、训练模型的逻辑。

关键在于代码的封装，这部分很有技巧，封装封的我看不懂代码。先把不封装代码写完，然后把各个部分抽出来放进函数就是封装了。

重要的一个知识点就是：pip安装的库函数，和他在github上的库代码结构是一摸一样的，通过pip安装比源码安装有更大的好处，因为通过pip安装可以在python中检索到，这就导致在任何地方都可以用官方的运行命令运行，哪怕使用`python -m`去测试他的模块也是可以的。而使用源码安装的，只能在源码所在的文件夹中使用`python -m`运行命令，在别的地方就不行了，所以推荐`pip`安装。

由于库函数的存在，使得编写代码更多的是库函数的形式，也就是使用封装函数的形式。在大模型领域，这个问题尤为常见。各种封装函数比如`pytorch`,`tensorflow`,`scikit-learn`等。

比如通过使用`flagembedding`库函数来加载模型：

```python
from FlagEmbedding import FlagReranker
reranker = FlagReranker('BAAI/bge-reranker-base', use_fp16=True) #use fp16 can speed up computing

score = reranker.compute_score(['query', 'passage'])
print(score)

scores = reranker.compute_score([['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']])
print(scores)
```

而这就导致了一个问题，实现同样功能的加载数据、加载模型的函数可能在不同的库函数中都存在，所以导致一个代码有多种调库的方式实现。比如上面的代码也可以用`huggingface`的`transformers`库函数来实现加载模型：

```python
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer, BatchEncoding, PreTrainedTokenizerFast

tokenizer = AutoTokenizer.from_pretrained('BAAI/bge-reranker-base')
model = AutoModelForSequenceClassification.from_pretrained('BAAI/bge-reranker-base')
model.eval()

pairs = [['what is panda?', 'hi'], ['what is panda?', 'The giant panda (Ailuropoda melanoleuca), sometimes called a panda bear or simply panda, is a bear species endemic to China.']]
with torch.no_grad():
    inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512)
    scores = model(**inputs, return_dict=True).logits.view(-1, ).float()
    print(scores)
```

这导致我对这个东西的运行迷迷糊糊的，又是`tokenizer`，又是`flagreranker`，不明白究竟这个代码是怎么运行的，尤其在没有弄明白代码的时候这个问题尤为严重。

在仔细观察以后发现，其实`flagreranker`也是调用的`transformers`的函数，只是又封装了一层。格外呆比。比如`FlagReranker`就对应着`autotokenizer`,`automodelforsequenceclassification`这行命令。

加载模型以后需要进行训练，也就是用模型对输入数据进行处理得到损失，利用损失对模型进行优化。封装可以分为小封装大封装，小封装就是各个模块单独封装，就比如上面对于加载模型的封装，大封装就是整个代码从加载数据到加载模型到训练模型都封装起来。比如：

```python
torchrun --nproc_per_node {number of gpus} \
-m FlagEmbedding.reranker.run \
--output_dir {path to save model} \
--model_name_or_path BAAI/bge-reranker-base \
--train_data ./toy_finetune_data.jsonl \
--learning_rate 6e-5 \
--fp16 \
--num_train_epochs 5 \
--per_device_train_batch_size {batch size; set 1 for toy data} \
--gradient_accumulation_steps 4 \
--dataloader_drop_last True \
--train_group_size 16 \
--max_len 512 \
--weight_decay 0.01 \
--logging_steps 10 
```

这就是一个大封装的代码。因为加载数据、加载模型、优化模型都在这个代码中完成了。其中加载模型部分我们已经分析过了，加载数据也无非就是使用数据类从本地路径中读入数据，关键在于如何训练模型。这部分我一直没仔细处理过。在这个代码中，训练模型使用了：

```python
_trainer_class = CETrainer
trainer = _trainer_class(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    data_collator=GroupCollator(tokenizer),
    tokenizer=tokenizer
)
Path(training_args.output_dir).mkdir(parents=True, exist_ok=True)
trainer.train()
trainer.save_model()
```

使用了trainer类，封装了训练模型、保存模型的相关代码。本质上仍就是是调用`transfomers`库函数的功能。

## 重新总结一下

代码安装可以使用源码安装或者包管理器比如`pip`、`conda`安装，包管理器安装的好处就是任何地方都能调用，源码安装就是可以清晰的看到代码，进而可以调用各个函数，缺点就是只能在文件夹路径下使用。

加载数据可以使用封装好的`transformers`中的`dataset`类，加载数据的逻辑也无非就是，读取文件路径，处理数据，转化为数字输出。

加载模型也可以使用库函数，背后的逻辑也是顺序指令的形式，根据路径名，加载模型，

上面的加载分为本地加载、云端加载、内存加载，云端加载无非就是先下载再加载，本地加载就是下载好了直接读取文件加载，内存加载就是已经加载好了，可以进一步使用模型训练模型了。

训练模型也可以使用封装好的`trainer`类，包括训练模型，评估模型，保存模型这三个顺序逻辑。这部分不是一次的，可能是循环顺序逻辑。即循环多次执行训练评估和保存。

这样子回头看这个`reranker`的代码确实清晰多了。看[Finetune cross-encoder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/reranker)最重要的收获就是明白了包管理器和源码安装之间的区别和共同点。尤其是pip的这个任何地方都可以看做把代码下载下来的特性也太有用了。

第二个知识点就是分清楚了全过程名词和分模块名词，比如说微调就是涵盖了加载数据和加载模型、训练模型的全过程，而处理数据只是对应了加载数据这个模块过程。目前相对来说能够熟练操作加载数据这个过程，还是使用的库函数，要是从c语言开始写肯定就很难。

第三点就是联想到我补全了那个node.js的代码也是一样的逻辑。并没有从c语言基础开始写，只是问`chatgpt`：在这个（node.js）编译环境中，要实现这个功能（获取在线链接中的内容，并读取第3行），该用什么函数？`chatgpt`给我提供了库函数及使用方法，我凭借经验进行了微调，最终得到了好的结果。

第四点内容就是直接跟label做运算的才是最终的损失，需要经过包含参数的函数转换才能跟label进行运算的都不是损失，这个含参数的函数应该也算做模型的一部分。直接跟label计算交叉熵或者最小二乘损失的才是预测结果。就算我们需要的是中间的特征层，也不影响我们用特征转换后的结果作为损失函数。

## 常用库函数名单

`pyspark`：可以在`python`中使用`spark`的相关命令比如`sql`等来从表格中获取数据。获取对象为数据帧`dataframe`，类似于`pandas`。
`pandas`：在`python`中读入文件为`dataframe`的形式。少了`sql`相关操作.

dataframe需要转化为numpy的ndarray才方便下一步的处理。也就是`df.values`，然后可以进一步转化为`list`，也就是`df.values.tolist()`

```python
def read_data(filename):
    #{"label":1,"query":"东大桥必胜客wx4g1vb5","pos":"必胜客（甜水园店）wx4g4s"}
    df = pd.read_json(filename,orient='records',lines=True)
    data1 = df[['query','pos']]
    label1=df['label']
    
    return data1.values.tolist(), label1.values.tolist()
```

`numpy`：使用pandas加载以后往往需要转化为numpy数组方便转化为dataset类处理。

bash系统中的循环语句和条件语句：`ls ~/dolphinfs_hdd_hadoop-search/hupenghui/llm_emb/exp_hope/training/ | grep $name | while read file;do case $file in $pattern) hope run $file;;*) echo "no";;esac;done`

按行读取文本并处理：``

ICL（in-context learning）

上下文学习也叫语境学习（不用关注名字的来源，只要知道研究的问题是什么就可以了）。

上下文学习主要是为了研究：为什么我们的神经网络模型会从之前的前馈神经网络变为transformer这种的架构。

![逆强化学习](icl.png){: w="400" h="300"}

更具体的来说，之前的网络都是基于输入样本的特征去预测这个样本的标签。而现在的transformer则是基于样本特征之间的相似性来预测样本的标签。从直觉上来说，transformer比之前的神经网络结构确实更加合理，但是为什么他比较合理，很少人分析过。

参考论文：https://arxiv.org/abs/2212.07677，https://arxiv.org/abs/2310.13220。

论文Transformers learn in-context by gradient descent是从梯度下降的角度来比较transformer和之前基于特征的网络之间的相同点和不同点。

​其实我们发散一下，也可以发现一个新的问题，就是transforemr架构是通过比较所有样本的特征之间的相似性来决定网络的输出的，那在放大一下，样本无穷多的情况下，这个比较的过程就会很漫长，这可以说是新的一类的信息检索问题，信息检索技术的提升方向往往就是tranformer模型架构的进化方向。信息检索技术的发展，可以用于改动神经网络的输入结构从而让神经网络的搜索效率逐渐增大。

具体分析过程可以参考：https://truenobility303.github.io/ICL-CL/。感觉没什么用，作者好像陷入dilemma了。不过也有可能破后而立，找到一种将transformer与前馈神经网络结合的方式。tranformer搜索范围大，前馈神经网络只关注自身特征。怎么即关注特征又减小搜索的空间呢？无非就是排序，对观察到的样本进行聚类排序，然后对类的代表性特征进行tranformer搜索。最终目的就是为了减小搜索的时间和空间耗费。怎么感觉无论做什么都是在原地踏步，好奇怪。物理学应该不是原地踏步吧。
